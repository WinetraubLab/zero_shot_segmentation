{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "Use this notebook to convert an OCT image to virtual histology.\n",
    "\n",
    "To get started,\n",
    "[open this notebook in colab](https://colab.research.google.com/github/WinetraubLab/zero_shot_segmentation/blob/main/point_based_segmentation.ipynb) and run.\n"
   ],
   "metadata": {
    "id": "NQOdtmQ3laV6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Up Environment"
   ],
   "metadata": {
    "id": "UNHZ8Sbinu1y"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "image_directory = '/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x OCT2Hist Model (Paper V2)/Results/TestSet/'"
   ],
   "metadata": {
    "id": "NtdbasoWVAwV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#segment anything (sam)\n",
    "using_colab = True\n",
    "visualize_sam_outputs = True\n",
    "\n",
    "#sam algorithm input parameters\n",
    "points_per_side=32\n",
    "pred_iou_thresh=0.90\n",
    "stability_score_thresh=0.95\n",
    "crop_n_layers=1\n",
    "crop_n_points_downscale_factor=2\n",
    "min_mask_region_area=3000\n",
    "\n",
    "#sam model type and weights checkpoint\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\""
   ],
   "metadata": {
    "id": "TLagUwFmXuN_"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#rf - roboflow dataset related params\n",
    "rf_api_key=\"R04BinsZcBZ6PsfKR2fP\"\n",
    "rf_workspace=\"yolab-kmmfx\"\n",
    "rf_project = \"connect_from_colab\"\n",
    "rf_dataset = \"png-mask-semantic\""
   ],
   "metadata": {
    "id": "HM7DPq3KXszb"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torchvision\n",
    "!git clone --recurse-submodules https://github.com/WinetraubLab/zero_shot_segmentation.git\n",
    "%run /content/zero_shot_segmentation/setup_roboflow_and_sam.ipynb\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "CHECKPOINT_PATH = \"/content/sam_vit_h_4b8939.pth\"\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
    "predictor = SamPredictor(sam)"
   ],
   "metadata": {
    "id": "rVzp13prYLGC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "!git clone --recurse-submodules https://github.com/WinetraubLab/OCT2Hist-ModelInference\n",
    "!pip install -r ./OCT2Hist-ModelInference/pytorch-CycleGAN-and-pix2pix/requirements.txt"
   ],
   "metadata": {
    "id": "KpOc9uoU27Ol"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sys.path.append('./zero_shot_segmentation')\n",
    "sys.path.append('./OCT2Hist-ModelInference')\n",
    "\n",
    "import oct2hist\n",
    "from utils.show_images import *\n",
    "# Set up the network environment\n",
    "oct2hist.setup_network()"
   ],
   "metadata": {
    "id": "ZI1JTVgfakkZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import oct2hist\n",
    "from utils.show_images import *\n",
    "from utils.crop import crop\n"
   ],
   "metadata": {
    "id": "XBw7pawsYD-f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "import os\n",
    "# List all files in the directory\n",
    "all_images = os.listdir(image_directory)\n",
    "\n",
    "# Filter out images without the suffix _realB\n",
    "filtered_images = [img for img in all_images if \"fake_B\" in img]\n",
    "random.shuffle(filtered_images)\n",
    "filtered_images = filtered_images[:10]\n",
    "\n",
    "# Uncomment this part if you would like to try a single image rather than all.\n",
    "#filtered_images = [''] # for one good example: ['LG-37-Slide06_Section02_yp0_patch01_real_A.png']"
   ],
   "metadata": {
    "id": "gT47QcMtUnS4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"run_oct2hist.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/github/WinetraubLab/OCT2Hist-ModelInference/blob/main/run_oct2hist.ipynb\n",
    "\n",
    "# Overview\n",
    "Use this notebook to convert an OCT image to virtual histology.\n",
    "\n",
    "To get started,\n",
    "[open this notebook in colab](https://colab.research.google.com/github/WinetraubLab/OCT2Hist-ModelInference/blob/main/run_oct2hist.ipynb) and run.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import sys\n",
    "from utils.show_images import showImg\n",
    "\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "import oct2hist\n",
    "from utils.masking import get_sam_input_points, show_points, show_mask, mask_gel_and_low_signal\n",
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "for filename in filtered_images:\n",
    "    oct_input_image_path = os.path.join(image_directory, filename)\n",
    "    # Load OCT image\n",
    "    oct_image = cv2.imread(oct_input_image_path)\n",
    "    oct_image = cv2.cvtColor(oct_image, cv2.COLOR_BGR2RGB)\n",
    "    #is it sheered?\n",
    "    right_column = oct_image.shape[1]-1\n",
    "    if (oct_image[:,0,0] == 0).all() or (oct_image[:,right_column,0] == 0).all():\n",
    "        continue\n",
    "    # OCT image's pixel size\n",
    "    microns_per_pixel_z = 1\n",
    "    microns_per_pixel_x = 1\n",
    "\n",
    "    # no need to crop - the current folder contains pre cropped images.\n",
    "    # cropped = crop_oct(oct_image)\n",
    "\n",
    "    #workaround: for some reason the images look close to the target shape, but not exactly.\n",
    "    oct_image = cv2.resize(oct_image, [1024, 512], interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    #for good input points, we need the gel masked out.\n",
    "    masked_gel_image = mask_gel_and_low_signal(oct_image)\n",
    "\n",
    "    # run vh&e\n",
    "    virtual_histology_image, _, o2h_input = oct2hist.run_network(oct_image,\n",
    "                                                                                             microns_per_pixel_x=microns_per_pixel_x,\n",
    "                                                                                             microns_per_pixel_z=microns_per_pixel_z)\n",
    "    # mask\n",
    "    input_point, input_label = get_sam_input_points(masked_image, virtual_histology_image)\n",
    "\n",
    "    predictor.set_image(virtual_histology_image)\n",
    "    masks, scores, logits = predictor.predict(point_coords=input_point, point_labels=input_label,\n",
    "                                              multimask_output=False, )\n",
    "\n",
    "    print(input_point, input_label, virtual_histology_image.shape)\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(virtual_histology_image)\n",
    "        show_mask(mask, plt.gca())\n",
    "        show_points(input_point, input_label, plt.gca())\n",
    "        plt.title(f\"Mask {i + 1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    print(f\"Image {filename} ready.\")\n"
   ],
   "metadata": {
    "id": "BOTPQ0McWWE0"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
