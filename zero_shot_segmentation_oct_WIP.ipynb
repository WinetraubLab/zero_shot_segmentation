{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "Use this notebook to convert an OCT image you have to an H&E image in order to evaluate how the code works.\n",
    "\n",
    "To get started,\n",
    "[open this notebook in colab](https://colab.research.google.com/github/WinetraubLab/zero_shot_segmentation/blob/main/zero_shot_segmentation_oct.ipynb)\n",
    " and run.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "239750a6e2ca3ff1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Path to an OCT image to convert\n",
    "oct_input_image_path = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Raw Data Used In Paper (Paper V2)/LG-19 - Slide04_Section02 (Fig 3.c)/OCTAligned.tiff\"\n",
    "\n",
    "#how many microns per pixel for each axis\n",
    "microns_per_pixel_z = 1\n",
    "microns_per_pixel_x = 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "effeb7673d4575c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#sam\n",
    "using_colab = True\n",
    "visualize_sam_outputs = True\n",
    "inject_real_histology_and_segmentation = True\n",
    "\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7c7e2cbaeb3915b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assumptions:\n",
    "\n",
    "oct scan x/z rates:\n",
    "*   microns per pixel z = 1\n",
    "*   microns per pixel x = 1\n",
    "\n",
    "pix2pix input sizes:\n",
    "*   virtual histology input width = 256\n",
    "*   virtual histology input height = 256\n",
    "\n",
    "pix2pix input x/z rates:\n",
    "*   microns per pixel z = 1\n",
    "*   microns per pixel x = 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68ddd5f731cc1237"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#pix2pix input sizes\n",
    "VIRTUAL_HIST_WIDTH = 256\n",
    "VIRTUAL_HIST_HEIGHT = 256\n",
    "#verify input sizes\n",
    "MICRONS_PER_PIXEL_Z_TARGET = 2\n",
    "MICRONS_PER_PIXEL_X_TARGET = 4"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d3673dc372e18ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#get roboflow input"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "kGFRW6dbJN9H"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3123d3673e103fe2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_oct2hist_outputs = False\n",
    "\n",
    "FIG_SIZE = (10,5)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "514f5313184bd005"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#installing pip requirements and git repos:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f3dfa8b051c527e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "init_dir = %pwd\n",
    "!pip install roboflow\n",
    "\n",
    "\n",
    "from IPython.display import clear_output\n",
    "# Clone repository\n",
    "!git clone --recurse-submodules https://github.com/WinetraubLab/OCT2Hist-UseModel\n",
    "\n",
    "base_folder = \"/content/rf_dir/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix\"\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r {base_folder}/requirements.txt\n",
    "# Clean up this window once install is complete\n",
    "clear_output()\n",
    "\n",
    "%cd init_dir"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "_ZsasJ44JKHg"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# inputs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc7aeee9ca83ac5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "XUNJ2Im6sGSJ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# oct2hist setup"
   ],
   "metadata": {
    "id": "Xal67p8B83EM"
   },
   "id": "Xal67p8B83EM"
  },
  {
   "cell_type": "code",
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# This is the folder that the pre-trained model is in\n",
    "model_folder = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Model (Paper V2)\"\n",
    "\n",
    "# Copy model to this folder over\n",
    "!mkdir {base_folder}/checkpoints\n",
    "!mkdir {base_folder}/checkpoints/pix2pix/\n",
    "!cp \"{model_folder}/latest_net_G.pth\" {base_folder}/checkpoints/pix2pix/\n",
    "!cp \"{model_folder}/latest_net_D.pth\" {base_folder}/checkpoints/pix2pix/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cWY3m7XcpDuz",
    "outputId": "8c9d4dbd-1491-4a95-d5b3-aa8f13bae909"
   },
   "id": "cWY3m7XcpDuz",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocess"
   ],
   "metadata": {
    "id": "8UTSh-cIpaMi"
   },
   "id": "8UTSh-cIpaMi"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load OCT image\n",
    "oct_image_orig = cv2.imread(oct_input_image_path)\n",
    "oct_image_orig = cv2.cvtColor(oct_image_orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "oct_image = oct_image_orig.copy()\n",
    "# Show Images to user\n",
    "fig, axes = plt.subplots(1, 2, figsize=FIG_SIZE)\n",
    "oct_image_orig_shape = oct_image.shape\n",
    "axes[0].imshow(oct_image)\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(f\"Original OCT image ({oct_image_orig_shape})\")\n",
    "\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "990f1eb977d4f9c8",
    "outputId": "ef36912f-e432-4061-89a0-d2d9bd4d996b"
   },
   "id": "990f1eb977d4f9c8"
  },
  {
   "cell_type": "code",
   "source": [
    "%cd /content/rf_dir/OCT2Hist-UseModel\n",
    "from utils.masking_utils import mask_image\n",
    "preprocessed_img, filt_img = mask_image(oct_image)"
   ],
   "metadata": {
    "id": "wWArPI6DplqL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "44b48e11-ad8a-475a-eefc-ab97571d5ad1"
   },
   "id": "wWArPI6DplqL",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from utils.img_utils import showImg\n",
    "showImg(preprocessed_img)"
   ],
   "metadata": {
    "id": "TK3SxeM-zgTt",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "outputId": "602b53b7-80ac-4f23-eb25-f38eabae9566"
   },
   "id": "TK3SxeM-zgTt",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "crop"
   ],
   "metadata": {
    "id": "BLCaB29bps8r"
   },
   "id": "BLCaB29bps8r"
  },
  {
   "cell_type": "code",
   "source": [
    "from utils.img_utils import showImg\n",
    "#slice from image\n",
    "width = 256 * 4\n",
    "height = 256 * 2\n",
    "x0 = 135\n",
    "z0= 350\n",
    "cropped = preprocessed_img[z0:z0+height, x0:x0+width]\n",
    "showImg(cropped)\n",
    "resized = cv2.resize(cropped, [VIRTUAL_HIST_WIDTH,VIRTUAL_HIST_HEIGHT] , interpolation=cv2.INTER_AREA)\n",
    "o2h_input = resized"
   ],
   "metadata": {
    "id": "WezE4f4-puKZ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "outputId": "a3f9bf6b-c907-4929-eb36-3670173c3d05"
   },
   "id": "WezE4f4-puKZ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "install dependencies required to read the image"
   ],
   "metadata": {
    "id": "Nz8QwLGlodOr"
   },
   "id": "Nz8QwLGlodOr"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "a1sJLt-Druw_"
   },
   "id": "a1sJLt-Druw_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "read it and verify it fits the input requirements."
   ],
   "metadata": {
    "id": "w71KS6WUookp"
   },
   "id": "w71KS6WUookp"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#run oct2hist"
   ],
   "metadata": {
    "collapsed": false,
    "id": "cf693b48a7221c75"
   },
   "id": "cf693b48a7221c75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a folder and place OCT image\n",
    "!mkdir {base_folder}/dataset\n",
    "!mkdir {base_folder}/dataset/test/\n",
    "\n",
    "# Before writting image to file, check size\n",
    "if o2h_input.shape[:2] != (256, 256):\n",
    "        raise ValueError(\"Image size must be 256x256 pixels to run model on.\")\n",
    "\n",
    "# Padd image and write it to the correct place\n",
    "padded = np.zeros([256,512,3], np.uint8)\n",
    "padded[:,:256,:] = o2h_input[:,:,:]\n",
    "cv2.imwrite(f\"{base_folder}/dataset/test/im1.jpg\", padded)"
   ],
   "metadata": {
    "id": "8d0d1401ba4dc8c2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d1b7cb81-ddd3-40fa-8c9d-f719dd05f44a"
   },
   "id": "8d0d1401ba4dc8c2"
  },
  {
   "cell_type": "code",
   "source": [
    "# This is the folder that the pre-trained model is in\n",
    "model_folder = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Model (Paper V2)\"\n",
    "\n",
    "# Copy model to this folder over\n",
    "!mkdir {base_folder}/checkpoints\n",
    "!mkdir {base_folder}/checkpoints/pix2pix/\n",
    "!cp \"{model_folder}/latest_net_G.pth\" {base_folder}/checkpoints/pix2pix/\n",
    "!cp \"{model_folder}/latest_net_D.pth\" {base_folder}/checkpoints/pix2pix/"
   ],
   "metadata": {
    "id": "DUUsOj8Kn4cs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cd09f17c-6742-4459-ab42-742feb5d23dc"
   },
   "id": "DUUsOj8Kn4cs",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!python {base_folder}/test.py --netG resnet_9blocks --dataroot \"{base_folder}/dataset/\"  --model pix2pix --name pix2pix --checkpoints_dir \"{base_folder}/checkpoints\" --results_dir \"{base_folder}/results\""
   ],
   "metadata": {
    "id": "79dc2be8da20b8f6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "87bcb1dc-a3b6-4fc3-be3e-72b42b01cce1"
   },
   "id": "79dc2be8da20b8f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Optional: visualize output"
   ],
   "metadata": {
    "id": "eQsbAkR6o4HO"
   },
   "id": "eQsbAkR6o4HO"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "histology_image = cv2.imread(f\"{base_folder}/results/pix2pix/test_latest/images/im1_fake_B.png\")\n",
    "histology_image = cv2.cvtColor(histology_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "height,width = cropped.shape[:2]\n",
    "histology_image_resized = cv2.resize(histology_image, [width,height] , interpolation=cv2.INTER_AREA)\n",
    "visualize_oct2hist_outputs = True\n",
    "if visualize_oct2hist_outputs:\n",
    "  # present side by side\n",
    "  fig, axes = plt.subplots(1, 2, figsize=FIG_SIZE)\n",
    "  axes[0].imshow(cropped)\n",
    "  axes[0].axis(\"off\")\n",
    "  axes[0].set_title(\"OCT\")\n",
    "  axes[1].imshow(histology_image_resized)\n",
    "  axes[1].axis(\"off\")\n",
    "  axes[1].set_title(\"Virtual Histology\")\n",
    "  plt.show()"
   ],
   "metadata": {
    "id": "8cf2f634b1edf4fc",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "outputId": "784d050d-7c62-4961-e0bb-21417e7ac742"
   },
   "id": "8cf2f634b1edf4fc"
  },
  {
   "cell_type": "code",
   "source": [
    "#inject ground truth histology\n",
    "histology_input_image_path = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Raw Data Used In Paper (Paper V2)/LG-19 - Slide04_Section02 (Fig 3.c)/HistologyAligned.tiff\"\n",
    "\n",
    "\n",
    "histology_image = cv2.imread(histology_input_image_path)\n",
    "histology_image = cv2.cvtColor(histology_image, cv2.COLOR_BGR2RGB)\n",
    "cropped_histology = histology_image[z0:z0+height, x0:x0+width]\n",
    "\n",
    "height,width = cropped.shape[:2]\n",
    "histology_image_resized = cv2.resize(cropped_histology, [width,height] , interpolation=cv2.INTER_AREA)\n",
    "visualize_oct2hist_outputs = True\n",
    "if visualize_oct2hist_outputs:\n",
    "  # present side by side\n",
    "  fig, axes = plt.subplots(1, 2, figsize=FIG_SIZE)\n",
    "  axes[0].imshow(cropped)\n",
    "  axes[0].axis(\"off\")\n",
    "  axes[0].set_title(\"OCT\")\n",
    "  axes[1].imshow(histology_image_resized)\n",
    "  axes[1].axis(\"off\")\n",
    "  axes[1].set_title(\"Virtual Histology\")\n",
    "  plt.show()\n"
   ],
   "metadata": {
    "id": "ByJ8nrZj-ilo",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "outputId": "ceb5ec44-c6c6-43ca-a9d8-c0c4e0635b04"
   },
   "id": "ByJ8nrZj-ilo",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#run sam on virtual histology"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d301ca9179966ba1"
   },
   "id": "d301ca9179966ba1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47e5a78f"
   },
   "source": [
    "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
   ],
   "id": "47e5a78f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1848a108"
   },
   "outputs": [],
   "source": [
    "from zero_shot_utils.utils import init_sam\n",
    "mask_generator =  init_sam(model_type,sam_checkpoint )\n"
   ],
   "id": "1848a108"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from zero_shot_utils.utils import get_roboflow_data\n",
    "!mkdir rf_dir\n",
    "get_roboflow_data()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "qCLPtMA-hTR9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"Torchvision version:\", torchvision.__version__)\n",
    "print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "import sys\n",
    "!{sys.executable} -m pip install opencv-python matplotlib\n",
    "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "\n",
    "!mkdir images\n",
    "!wget -P images https://pbs.twimg.com/media/FvpQj7UWYAAgxfo?format=jpg&name=large\n",
    "#https://twitter.com/JMGardnerMD/status/1655724394805706752/photo/1\n",
    "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "0685a2f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "yty-ZZqTJkG7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set-up"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd2bc687"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "560725a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "\n",
    "  if len(anns) == 0:\n",
    "    return\n",
    "  sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_autoscale_on(False)\n",
    "\n",
    "  img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "  img[:,:,3] = 0\n",
    "  for ann in sorted_anns:\n",
    "      m = ann['segmentation']\n",
    "      color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "      img[m] = color_mask\n",
    "  ax.imshow(img)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74b6e5f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Example image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27c41445"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "Ndp026QrNVHB"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Automatic mask generation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c2824a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9ef74c5"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6b1ea21"
   },
   "source": [
    "To generate masks, just run `generate` on an image."
   ],
   "id": "d6b1ea21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "391771c1"
   },
   "outputs": [],
   "source": [
    "masks = mask_generator.generate(histology_image_resized)"
   ],
   "id": "391771c1"
  },
  {
   "cell_type": "code",
   "source": [
    "if visualize_sam_outputs:\n",
    "  plt.figure(figsize=FIG_SIZE)\n",
    "  plt.imshow(histology_image_resized)\n",
    "  show_anns(masks)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ],
   "metadata": {
    "id": "eZbgErUlAqDz",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "outputId": "0f16b027-88cb-4c42-b7ea-fe575ef2d388"
   },
   "id": "eZbgErUlAqDz",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e36a1a39"
   },
   "source": [
    "Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:\n",
    "* `segmentation` : the mask\n",
    "* `area` : the area of the mask in pixels\n",
    "* `bbox` : the boundary box of the mask in XYWH format\n",
    "* `predicted_iou` : the model's own prediction for the quality of the mask\n",
    "* `point_coords` : the sampled input point that generated this mask\n",
    "* `stability_score` : an additional measure of mask quality\n",
    "* `crop_box` : the crop of the image used to generate this mask in XYWH format"
   ],
   "id": "e36a1a39"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53009a1f"
   },
   "source": [
    "Show all the masks overlayed on the image."
   ],
   "id": "53009a1f"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00b3d6b2"
   },
   "source": [
    "## Automatic mask generation options"
   ],
   "id": "00b3d6b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "183de84e"
   },
   "source": [
    "There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:[link text](https://)\n"
   ],
   "id": "183de84e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WMuUj8CUJSa"
   },
   "outputs": [],
   "source": [],
   "id": "4WMuUj8CUJSa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inject ground truth: the true h&e image:"
   ],
   "metadata": {
    "id": "ym7CSVK7lCBG"
   },
   "id": "ym7CSVK7lCBG"
  },
  {
   "cell_type": "code",
   "source": [
    "from zero_shot_utils.utils import sam_masking\n",
    "masks2 = sam_masking(inject_real_histology_and_segmentation = True)"
   ],
   "metadata": {
    "id": "lwNBKpD5lAdH"
   },
   "id": "lwNBKpD5lAdH",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb702ae3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "outputId": "834f29ad-f192-4e0b-c1d4-bab88fc5e91f"
   },
   "outputs": [],
   "source": [
    "if visualize_sam_outputs:\n",
    "  plt.figure(figsize=FIG_SIZE)\n",
    "  plt.imshow(histology_image_resized)\n",
    "  show_anns(masks2)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ],
   "id": "fb702ae3"
  },
  {
   "cell_type": "code",
   "source": [
    "if visualize_sam_outputs:\n",
    "  "
   ],
   "metadata": {
    "id": "3VJQrMwsBJQk",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "ed774ff1-51e0-4d0b-8f42-ab27febdb64a"
   },
   "id": "3VJQrMwsBJQk",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#project on oct"
   ],
   "metadata": {
    "id": "21jMiU5lpQdW"
   },
   "id": "21jMiU5lpQdW"
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=FIG_SIZE)\n",
    "plt.imshow(cropped)\n",
    "show_anns(masks2)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "m25KFnmrpVeh",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "outputId": "e583c663-a049-47f8-e764-fb8e8a48f8d4"
   },
   "id": "m25KFnmrpVeh",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Eval results"
   ],
   "metadata": {
    "id": "4kuS5Ho6AxPt"
   },
   "id": "4kuS5Ho6AxPt"
  },
  {
   "cell_type": "code",
   "source": [
    "from zero_shot_utils.utils import score_masking\n",
    "# Replace with the path to your segmentation mask file\n",
    "segmentation_mask_path = f\"/content/rf_dir/Zero-shot-oct-1/train/Hist-1_png.rf.168d6c48c79ccf974a2a1ecac761d3f5_mask.png\"\n",
    "# Load the segmentation mask image using OpenCV\n",
    "segmentation_mask = cv2.imread(segmentation_mask_path, cv2.IMREAD_UNCHANGED)\n",
    "print(score_masking(masks2, segmentation_mask))\n"
   ],
   "metadata": {
    "id": "QNFS33Nm-eNm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0e19e446-4db4-4738-ae65-8143ad21ee98"
   },
   "id": "QNFS33Nm-eNm",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
