{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "Use this notebook to convert an OCT image to virtual histology.\n",
        "\n",
        "To get started,\n",
        "[open this notebook in colab](https://colab.research.google.com/github/WinetraubLab/zero_shot_segmentation/blob/main/measure_point_based_segmentation.ipynb) and run.\n"
      ],
      "metadata": {
        "id": "NQOdtmQ3laV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up Environment"
      ],
      "metadata": {
        "id": "UNHZ8Sbinu1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "image_directory = '/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x OCT2Hist Model (Paper V2)/Results/TestSet/'"
      ],
      "metadata": {
        "id": "NtdbasoWVAwV",
        "outputId": "04b53c89-91e0-4b97-ca49-ef0ca360edb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#segment anything (sam)\n",
        "using_colab = True\n",
        "visualize_sam_outputs = True\n",
        "\n",
        "#sam algorithm input parameters\n",
        "points_per_side=32\n",
        "pred_iou_thresh=0.90\n",
        "stability_score_thresh=0.95\n",
        "crop_n_layers=1\n",
        "crop_n_points_downscale_factor=2\n",
        "min_mask_region_area=3000\n",
        "\n",
        "#sam model type and weights checkpoint\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\""
      ],
      "metadata": {
        "id": "TLagUwFmXuN_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_api_key= \"R04BinsZcBZ6PsfKR2fP\"\n",
        "rf_workspace= \"yolab-kmmfx\"\n",
        "rf_project = \"11-16-2023-zero-shot-oct\"\n",
        "rf_dataset = \"coco-segmentation\" #\"png-mask-semantic\"\n",
        "rf_version = 3"
      ],
      "metadata": {
        "id": "HM7DPq3KXszb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "!git clone --recurse-submodules https://github.com/WinetraubLab/zero_shot_segmentation.git\n",
        "%run /content/zero_shot_segmentation/setup_roboflow_and_sam.ipynb\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda')\n",
        "MODEL_TYPE = \"vit_h\"\n",
        "CHECKPOINT_PATH = \"/content/sam_vit_h_4b8939.pth\"\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "id": "Z8VFd8xvedrb",
        "outputId": "b10e1cd4-a1b4-4e50-97aa-9c16b6163cd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'zero_shot_segmentation' already exists and is not an empty directory.\n",
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.10/dist-packages (1.1.9)\n",
            "Requirement already satisfied: certifi==2023.7.22 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2023.7.22)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.23.5)\n",
            "Requirement already satisfied: opencv-python-headless==4.8.0.74 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.8.0.74)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (9.4.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.16.0)\n",
            "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.0.7)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0.1)\n",
            "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.0.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from roboflow) (0.4.27)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.2.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.44.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (3.3.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from supervision->roboflow) (1.11.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#roboflow semantic classes\n",
        "EPIDERMIS = True #mask values for epidermis mask\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image, ImageDraw"
      ],
      "metadata": {
        "id": "ZF5reKUYdMow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def coco_mask_to_numpy(image_shape, coco_mask):\n",
        "    \"\"\"\n",
        "    Convert COCO format segmentation mask to a NumPy array.\n",
        "\n",
        "    Parameters:\n",
        "    - image_shape: Tuple (m, n) representing the shape of the image.\n",
        "    - coco_mask: List of coordinates [x1, y1, x2, y2, ..., xn, yn] in COCO format.\n",
        "\n",
        "    Returns:\n",
        "    - numpy_mask: NumPy array of shape (m, n) with True within the mask boundaries and False elsewhere.\n",
        "    \"\"\"\n",
        "    # Create an image and draw the polygon defined by the COCO mask\n",
        "    mask_image = Image.new(\"1\", image_shape[::-1], 0)\n",
        "    draw = ImageDraw.Draw(mask_image)\n",
        "    draw.polygon(coco_mask, outline=1, fill=1)\n",
        "    del draw\n",
        "\n",
        "    # Convert the mask image to a NumPy array\n",
        "    numpy_mask = np.array(mask_image, dtype=bool)\n",
        "\n",
        "    return numpy_mask"
      ],
      "metadata": {
        "id": "jLqDaB9udQ3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_images_and_masks(api_key, workspace, project_name, dataset_name, version):\n",
        "    from roboflow import Roboflow\n",
        "    rf = Roboflow(api_key=api_key)\n",
        "    project = rf.workspace(workspace).project(project_name)\n",
        "    dataset = project.version(version).download(rf_dataset_type, overwrite = False)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "0jBCUUOidTOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(mask_true, mask_pred, class_id):\n",
        "    #intersection = np.logical_and(mask_true == class_id, mask_pred == class_id)\n",
        "    intersection = np.logical_and(mask_true, mask_pred == class_id)\n",
        "    union = np.logical_or(mask_true, mask_pred == class_id)\n",
        "\n",
        "    class_iou = np.sum(intersection) / np.sum(union)\n",
        "    return class_iou"
      ],
      "metadata": {
        "id": "Ors-6o-idUVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download images and masks\n",
        "dataset = download_images_and_masks(rf_api_key, rf_workspace, rf_project_name, rf_dataset_type, version)\n",
        "# prepare model\n",
        "DEVICE = torch.device('mps')  # 'cpu'\n",
        "MODEL_TYPE = \"vit_h\"\n",
        "CHECKPOINT_PATH = \"/Users/dannybarash/Code/oct/zero_shot_segmentation_test_sam/weights/sam_vit_h_4b8939.pth\"  # os.path.join(\"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)\n",
        "predictor = SamPredictor(sam)\n",
        "\n",
        "\n",
        "total_iou = 0\n",
        "total_samples = 0\n",
        "annot_dataset_dir = \"/Users/dannybarash/Code/oct/zero_shot_segmentation_test_sam/zero_shot_segmentation/11/16/2023-Zero-shot-OCT-3/test/\"\n",
        "raw_oct_dataset_dir = \"/Users/dannybarash/Library/CloudStorage/GoogleDrive-dannybarash7@gmail.com/Shared drives/Yolab - Current Projects/Yonatan/Hist Images/\"\n",
        "real_histology_dir = raw_oct_dataset_dir\n",
        "# Get the list of image files\n",
        "image_files = [f for f in os.listdir(annot_dataset_dir) if f.endswith(\".jpg\")]\n",
        "image_files = image_files[:2]\n",
        "\n",
        "total_iou = {EPIDERMIS:0}  # DERMIS:0 , # IOU for each class\n",
        "total_iou_oct = {EPIDERMIS:0}\n",
        "total_samples = 0\n",
        "path_to_annotations = os.path.join(annot_dataset_dir, \"_annotations.coco.json\")\n",
        "from pylabel import importer\n",
        "dataset = importer.ImportCoco(path_to_annotations, path_to_images=annot_dataset_dir, name=\"zero_shot_oct\")\n",
        "visualize = False\n",
        "segment_oct = True\n",
        "segment_real_hist = False"
      ],
      "metadata": {
        "id": "2vPVox_OdUWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_file in tqdm(image_files):\n",
        "    gt_image_path = os.path.join(raw_oct_dataset_dir, image_file)\n",
        "    image_path = os.path.join(annot_dataset_dir, image_file)\n",
        "    oct_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    coco_mask = dataset.df.ann_segmentation[dataset.df.img_filename == image_file].values[0][0]\n",
        "    mask_true = coco_mask_to_numpy(oct_img.shape, coco_mask)\n",
        "    if visualize:\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(oct_img, cmap = \"gray\")\n",
        "        show_mask(mask_true, plt.gca())\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Input oct and annotation: {image_file}\")\n",
        "        plt.show()\n",
        "\n",
        "    # Predict the mask using your model\n",
        "    mask, masked_gel_image, crop_args = predict(image_path, predictor, weights_path = CHECKPOINT_PATH)\n",
        "\n",
        "    if segment_oct:\n",
        "        oct_mask, _, crop_args = predict(image_path, predictor, weights_path=CHECKPOINT_PATH, vhist=False)\n",
        "\n",
        "    # if segment_real_hist:\n",
        "    #     image_path = os.path.join(real_histology_dir, image_file)\n",
        "    #     oct_mask, _, crop_args = predict(image_path, predictor, weights_path=CHECKPOINT_PATH, vhist=False)\n",
        "\n",
        "    if mask is None or mask.sum().sum()==0:\n",
        "        print(f\"Could not segment {image_path}.\")\n",
        "        continue\n",
        "    mask[mask==1] = True\n",
        "    mask[mask == 0] = False\n",
        "    #mask_pred = cv2.resize(mask_pred, (mask_true.shape[1], mask_true.shape[0]), interpolation =  cv2.INTER_NEAREST)\n",
        "    cropped_mask_gt = crop(mask_true, **crop_args)\n",
        "    cropped_oct_image = crop(oct_img, **crop_args)\n",
        "    from PIL import Image\n",
        "\n",
        "    if visualize:\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(cropped_oct_image, cmap = \"gray\")\n",
        "        show_mask(mask, plt.gca())\n",
        "        show_mask(cropped_mask_gt, plt.gca(), random_color=True)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Cropped oct and segmentation: {image_file}\")\n",
        "        plt.show()\n",
        "\n",
        "    # Calculate IoU for each class\n",
        "    for class_id in [EPIDERMIS]: # DERMIS\n",
        "        epidermis_iou_vhist = calculate_iou(cropped_mask_gt, mask, class_id)\n",
        "        if segment_oct:\n",
        "            epidermis_iou_oct = calculate_iou(cropped_mask_gt, oct_mask, class_id)\n",
        "            total_iou_oct[class_id] += epidermis_iou_oct\n",
        "        total_iou[class_id] += epidermis_iou_vhist\n",
        "\n",
        "    total_samples += 1\n"
      ],
      "metadata": {
        "id": "g_6Dp5_leZsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "average_iou = total_iou[EPIDERMIS] / total_samples #sum all ious divided by (number of images * number of classes).\n",
        "print(f\"Average IoU with virtual histology: {average_iou}\")\n",
        "if segment_oct:\n",
        "    average_iou_oct = total_iou_oct[EPIDERMIS] / total_samples\n",
        "    print(f\"Average IoU without virtual histology: {average_iou_oct}\")"
      ],
      "metadata": {
        "id": "GukN39pLeay2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rVzp13prYLGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --recurse-submodules https://github.com/WinetraubLab/OCT2Hist-ModelInference\n",
        "!pip install -r ./OCT2Hist-ModelInference/pytorch-CycleGAN-and-pix2pix/requirements.txt"
      ],
      "metadata": {
        "id": "KpOc9uoU27Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append('./zero_shot_segmentation')\n",
        "sys.path.append('./OCT2Hist-ModelInference')\n",
        "\n",
        "import oct2hist\n",
        "from utils.show_images import *\n",
        "# Set up the network environment\n",
        "oct2hist.setup_network()"
      ],
      "metadata": {
        "id": "ZI1JTVgfakkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import oct2hist\n",
        "from utils.show_images import *\n",
        "from utils.crop import crop\n"
      ],
      "metadata": {
        "id": "XBw7pawsYD-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "# List all files in the directory\n",
        "all_images = os.listdir(image_directory)\n",
        "\n",
        "# Filter out images without the suffix _realB\n",
        "filtered_images = [img for img in all_images if \"real_A\" in img]\n",
        "random.shuffle(filtered_images)\n",
        "filtered_images = filtered_images[:10]\n",
        "\n",
        "# Uncomment this part if you would like to try a single image rather than all.\n",
        "#filtered_images = [''] # for one good example: ['LG-37-Slide06_Section02_yp0_patch01_real_A.png']"
      ],
      "metadata": {
        "id": "gT47QcMtUnS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"run_oct2hist.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/github/WinetraubLab/OCT2Hist-ModelInference/blob/main/run_oct2hist.ipynb\n",
        "\n",
        "# Overview\n",
        "Use this notebook to convert an OCT image to virtual histology.\n",
        "\n",
        "To get started,\n",
        "[open this notebook in colab](https://colab.research.google.com/github/WinetraubLab/OCT2Hist-ModelInference/blob/main/run_oct2hist.ipynb) and run.\n",
        "\"\"\"\n",
        "\n",
        "import random\n",
        "import sys\n",
        "from utils.show_images import showImg\n",
        "\n",
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "import oct2hist\n",
        "from utils.masking import get_sam_input_points, show_points, show_mask, mask_gel_and_low_signal\n",
        "\n",
        "# from google.colab import drive\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "\n",
        "for filename in filtered_images:\n",
        "    oct_input_image_path = os.path.join(image_directory, filename)\n",
        "    # Load OCT image\n",
        "    oct_image = cv2.imread(oct_input_image_path)\n",
        "    oct_image = cv2.cvtColor(oct_image, cv2.COLOR_BGR2RGB)\n",
        "    #is it sheered?\n",
        "    right_column = oct_image.shape[1]-1\n",
        "    if (oct_image[:,0,0] == 0).all() or (oct_image[:,right_column,0] == 0).all():\n",
        "        continue\n",
        "    # OCT image's pixel size\n",
        "    microns_per_pixel_z = 1\n",
        "    microns_per_pixel_x = 1\n",
        "\n",
        "    # no need to crop - the current folder contains pre cropped images.\n",
        "    # cropped = crop_oct(oct_image)\n",
        "\n",
        "    #workaround: for some reason the images look close to the target shape, but not exactly.\n",
        "    oct_image = cv2.resize(oct_image, [1024, 512], interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    #for good input points, we need the gel masked out.\n",
        "    masked_gel_image = mask_gel_and_low_signal(oct_image)\n",
        "\n",
        "    # run vh&e\n",
        "    virtual_histology_image, _, o2h_input = oct2hist.run_network(oct_image,\n",
        "                                                                                             microns_per_pixel_x=microns_per_pixel_x,\n",
        "                                                                                             microns_per_pixel_z=microns_per_pixel_z)\n",
        "    # mask\n",
        "    input_point, input_label = get_sam_input_points(masked_gel_image, virtual_histology_image)\n",
        "\n",
        "    predictor.set_image(virtual_histology_image)\n",
        "    masks, scores, logits = predictor.predict(point_coords=input_point, point_labels=input_label,\n",
        "                                              multimask_output=False, )\n",
        "\n",
        "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(virtual_histology_image)\n",
        "        show_mask(mask, plt.gca())\n",
        "        show_points(input_point, input_label, plt.gca())\n",
        "        plt.title(f\"Mask {i + 1}, Score: {score:.3f}\", fontsize=18)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "    print(f\"Image {filename} ready.\")\n"
      ],
      "metadata": {
        "id": "BOTPQ0McWWE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L-AUthzH65lL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}