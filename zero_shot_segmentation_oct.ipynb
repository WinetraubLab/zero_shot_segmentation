{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "Use this notebook to convert an OCT image you have to an H&E image in order to evaluate how the code works.\n",
        "\n",
        "To get started,\n",
        "[open this notebook in colab](https://colab.research.google.com/github/WinetraubLab/zero_shot_segmentation/blob/main/zero_shot_segmentation_oct.ipynb)\n",
        " and run.\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "239750a6e2ca3ff1"
      },
      "id": "239750a6e2ca3ff1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# inputs"
      ],
      "metadata": {
        "collapsed": false,
        "id": "cc7aeee9ca83ac5b"
      },
      "id": "cc7aeee9ca83ac5b"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": [
        "# Path to an OCT image to convert\n",
        "oct_input_image_path = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Raw Data Used In Paper (Paper V2)/LG-19 - Slide04_Section02 (Fig 3.c)/OCTAligned.tiff\"\n",
        "\n",
        "#how many microns per pixel for each axis\n",
        "microns_per_pixel_z = 1\n",
        "microns_per_pixel_x = 1"
      ],
      "metadata": {
        "id": "effeb7673d4575c4"
      },
      "id": "effeb7673d4575c4"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "XUNJ2Im6sGSJ"
      },
      "id": "XUNJ2Im6sGSJ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": [
        "visualize_oct2hist_outputs = False\n",
        "\n",
        "FIG_SIZE = (10,5)"
      ],
      "metadata": {
        "id": "JZj8621TpY5g"
      },
      "id": "JZj8621TpY5g"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "#rf\n",
        "rf_api_key=\"R04BinsZcBZ6PsfKR2fP\"\n",
        "rf_workspace=\"yolab-kmmfx\"\n",
        "rf_project = \"connect_from_colab\"\n",
        "rf_dataset = \"png-mask-semantic\"\n"
      ],
      "metadata": {
        "id": "4fe300fb"
      },
      "id": "4fe300fb"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "#sam\n",
        "using_colab = True\n",
        "visualize_sam_outputs = True\n",
        "\n",
        "#sam\n",
        "points_per_side=32\n",
        "pred_iou_thresh=0.90\n",
        "stability_score_thresh=0.95\n",
        "crop_n_layers=1\n",
        "crop_n_points_downscale_factor=2\n",
        "min_mask_region_area=3000\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\""
      ],
      "metadata": {
        "id": "F9UF0zHVUZWA"
      },
      "id": "F9UF0zHVUZWA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assumptions:\n",
        "\n",
        "oct scan x/z rates:\n",
        "*   microns per pixel z = 1\n",
        "*   microns per pixel x = 1\n",
        "\n",
        "pix2pix input sizes:\n",
        "*   virtual histology input width = 256\n",
        "*   virtual histology input height = 256\n",
        "\n",
        "pix2pix input x/z rates:\n",
        "*   microns per pixel z = 1\n",
        "*   microns per pixel x = 2"
      ],
      "metadata": {
        "collapsed": false,
        "id": "uOKT0inNrHLJ"
      },
      "id": "uOKT0inNrHLJ"
    },
    {
      "cell_type": "code",
      "source": [
        "#pix2pix input sizes\n",
        "VIRTUAL_HIST_WIDTH = 256\n",
        "VIRTUAL_HIST_HEIGHT = 256\n",
        "#verify input sizes\n",
        "MICRONS_PER_PIXEL_Z_TARGET = 2\n",
        "MICRONS_PER_PIXEL_X_TARGET = 4"
      ],
      "metadata": {
        "id": "031q6iphqkCP"
      },
      "id": "031q6iphqkCP",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#non python, colab setup code"
      ],
      "metadata": {
        "collapsed": false,
        "id": "71a9a28163d8c4c9"
      },
      "id": "71a9a28163d8c4c9"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Clone repository\n",
        "!git clone --recurse-submodules https://github.com/WinetraubLab/OCT2Hist-UseModel\n",
        "\n",
        "base_folder = \"/content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix\"\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r {base_folder}/requirements.txt\n",
        "\n",
        "# Clean up this window once install is complete\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niF5tEhPo8lX",
        "outputId": "16aae290-be16-4623-da52-3e07ecc558e7"
      },
      "id": "niF5tEhPo8lX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'OCT2Hist-UseModel'...\n",
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (97/97), done.\u001b[K\n",
            "remote: Total 141 (delta 71), reused 71 (delta 30), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (141/141), 5.79 MiB | 4.26 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "Submodule 'pytorch-CycleGAN-and-pix2pix' (https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix) registered for path 'pytorch-CycleGAN-and-pix2pix'\n",
            "Cloning into '/content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2513, done.        \n",
            "remote: Total 2513 (delta 0), reused 0 (delta 0), pack-reused 2513        \n",
            "Receiving objects: 100% (2513/2513), 8.20 MiB | 29.76 MiB/s, done.\n",
            "Resolving deltas: 100% (1575/1575), done.\n",
            "Submodule path 'pytorch-CycleGAN-and-pix2pix': checked out '9f8f61e5a375c2e01c5187d093ce9c2409f409b0'\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 2)) (0.15.2+cu118)\n",
            "Collecting dominate>=2.4.0 (from -r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 3))\n",
            "  Downloading dominate-2.8.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting visdom>=0.1.8.8 (from -r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 4))\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb (from -r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5))\n",
            "  Downloading wandb-0.15.8-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.5.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 4)) (1.10.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 4)) (6.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 4)) (1.16.0)\n",
            "Collecting jsonpatch (from visdom>=0.1.8.8->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 4))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from visdom>=0.1.8.8->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 4)) (1.6.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5)) (8.1.6)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5))\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5)) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5))\n",
            "  Downloading sentry_sdk-1.29.2-py2.py3-none-any.whl (215 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5)) (6.0.1)\n",
            "Collecting pathtools (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5))\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5)) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5)) (3.20.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5))\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 2)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 2)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.5.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (2.1.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch->visdom>=0.1.8.8->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 4))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 1)) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->-r /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/requirements.txt (line 5))\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: visdom, pathtools\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408197 sha256=971d710baa58a76273396bf59db2576f84d2745a34493dc8e6adcf95d9e176ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/29/49/5bed207bac4578e4d2c0c5fc0226bfd33a7e2953ea56356855\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=caf217090dd757115257b649a5e9a704f4688f27158dbdef72c8f9cd5c1061a2\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built visdom pathtools\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# This is the folder that the pre-trained model is in\n",
        "model_folder = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Model (Paper V2)\"\n",
        "\n",
        "# Copy model to this folder over\n",
        "!mkdir {base_folder}/checkpoints\n",
        "!mkdir {base_folder}/checkpoints/pix2pix/\n",
        "!cp \"{model_folder}/latest_net_G.pth\" {base_folder}/checkpoints/pix2pix/\n",
        "!cp \"{model_folder}/latest_net_D.pth\" {base_folder}/checkpoints/pix2pix/"
      ],
      "metadata": {
        "id": "cWY3m7XcpDuz"
      },
      "id": "cWY3m7XcpDuz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess"
      ],
      "metadata": {
        "id": "8UTSh-cIpaMi"
      },
      "id": "8UTSh-cIpaMi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load OCT image\n",
        "oct_image_orig = cv2.imread(oct_input_image_path)\n",
        "oct_image_orig = cv2.cvtColor(oct_image_orig, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "oct_image = oct_image_orig.copy()\n",
        "# Show Images to user\n",
        "fig, axes = plt.subplots(1, 2, figsize=FIG_SIZE)\n",
        "oct_image_orig_shape = oct_image.shape\n",
        "axes[0].imshow(oct_image)\n",
        "axes[0].axis(\"off\")\n",
        "axes[0].set_title(f\"Original OCT image ({oct_image_orig_shape})\")\n",
        "\n"
      ],
      "metadata": {
        "id": "990f1eb977d4f9c8"
      },
      "id": "990f1eb977d4f9c8"
    },
    {
      "cell_type": "code",
      "source": [
        "#update the image to be 2mic/pix on the x axis, and 1mic/pix on the y axis.\n",
        "new_image_width = int(oct_image.shape[1] * microns_per_pixel_x / MICRONS_PER_PIXEL_X_TARGET)\n",
        "new_image_height = int(oct_image.shape[0] * microns_per_pixel_z / MICRONS_PER_PIXEL_Z_TARGET)\n",
        "\n",
        "# if new_image_width<VIRTUAL_HIST_WIDTH or VIRTUAL_HIST_HEIGHT<256:\n",
        "#   print(f\"Image at target x/z rate is smaller than ({VIRTUAL_HIST_HEIGHT},{VIRTUAL_HIST_WIDTH}), and will be placed at top left corner.\")\n",
        "# #warn about cropping\n",
        "# if new_image_width>VIRTUAL_HIST_WIDTH or new_image_height>VIRTUAL_HIST_HEIGHT:\n",
        "#   raise Exception(f\"Image at target x/z rate is too large: ({new_image_height},{new_image_width}), please adjust so that it won't be larger than ({VIRTUAL_HIST_HEIGHT},{VIRTUAL_HIST_WIDTH}).\")"
      ],
      "metadata": {
        "id": "P_TXQb-bpMCx"
      },
      "id": "P_TXQb-bpMCx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/OCT2Hist-UseModel\n",
        "from utils.masking_utils import mask_image\n",
        "preprocessed_img, filt_img = mask_image(oct_image)"
      ],
      "metadata": {
        "id": "wWArPI6DplqL"
      },
      "id": "wWArPI6DplqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.img_utils import showImg\n",
        "showImg(preprocessed_img)"
      ],
      "metadata": {
        "id": "TK3SxeM-zgTt"
      },
      "id": "TK3SxeM-zgTt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "crrop"
      ],
      "metadata": {
        "id": "BLCaB29bps8r"
      },
      "id": "BLCaB29bps8r"
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.img_utils import showImg\n",
        "#slice from image\n",
        "width = 256 * 4\n",
        "height = 256 * 2\n",
        "x0 = 135\n",
        "z0= 350\n",
        "cropped = preprocessed_img[z0:z0+height, x0:x0+width]\n",
        "showImg(cropped)\n",
        "resized = cv2.resize(cropped, [VIRTUAL_HIST_WIDTH,VIRTUAL_HIST_HEIGHT] , interpolation=cv2.INTER_AREA)\n",
        "o2h_input = resized"
      ],
      "metadata": {
        "id": "WezE4f4-puKZ"
      },
      "id": "WezE4f4-puKZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "install dependencies required to read the image"
      ],
      "metadata": {
        "id": "Nz8QwLGlodOr"
      },
      "id": "Nz8QwLGlodOr"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1sJLt-Druw_"
      },
      "id": "a1sJLt-Druw_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "read it and verify it fits the input requirements."
      ],
      "metadata": {
        "id": "w71KS6WUookp"
      },
      "id": "w71KS6WUookp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#run oct2hist"
      ],
      "metadata": {
        "collapsed": false,
        "id": "cf693b48a7221c75"
      },
      "id": "cf693b48a7221c75"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create a folder and place OCT image\n",
        "!mkdir {base_folder}/dataset\n",
        "!mkdir {base_folder}/dataset/test/\n",
        "\n",
        "# Before writting image to file, check size\n",
        "if o2h_input.shape[:2] != (256, 256):\n",
        "        raise ValueError(\"Image size must be 256x256 pixels to run model on.\")\n",
        "\n",
        "# Padd image and write it to the correct place\n",
        "padded = np.zeros([256,512,3], np.uint8)\n",
        "padded[:,:256,:] = o2h_input[:,:,:]\n",
        "cv2.imwrite(f\"{base_folder}/dataset/test/im1.jpg\", padded)"
      ],
      "metadata": {
        "id": "8d0d1401ba4dc8c2"
      },
      "id": "8d0d1401ba4dc8c2"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the folder that the pre-trained model is in\n",
        "model_folder = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Model (Paper V2)\"\n",
        "\n",
        "# Copy model to this folder over\n",
        "!mkdir {base_folder}/checkpoints\n",
        "!mkdir {base_folder}/checkpoints/pix2pix/\n",
        "!cp \"{model_folder}/latest_net_G.pth\" {base_folder}/checkpoints/pix2pix/\n",
        "!cp \"{model_folder}/latest_net_D.pth\" {base_folder}/checkpoints/pix2pix/"
      ],
      "metadata": {
        "id": "DUUsOj8Kn4cs"
      },
      "id": "DUUsOj8Kn4cs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "!python {base_folder}/test.py --netG resnet_9blocks --dataroot \"{base_folder}/dataset/\"  --model pix2pix --name pix2pix --checkpoints_dir \"{base_folder}/checkpoints\" --results_dir \"{base_folder}/results\""
      ],
      "metadata": {
        "id": "79dc2be8da20b8f6"
      },
      "id": "79dc2be8da20b8f6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optional: visualize output"
      ],
      "metadata": {
        "id": "eQsbAkR6o4HO"
      },
      "id": "eQsbAkR6o4HO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "histology_image = cv2.imread(f\"{base_folder}/results/pix2pix/test_latest/images/im1_fake_B.png\")\n",
        "histology_image = cv2.cvtColor(histology_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "height,width = cropped.shape[:2]\n",
        "histology_image_resized = cv2.resize(histology_image, [width,height] , interpolation=cv2.INTER_AREA)\n",
        "visualize_oct2hist_outputs = True\n",
        "if visualize_oct2hist_outputs:\n",
        "  # present side by side\n",
        "  fig, axes = plt.subplots(1, 2, figsize=FIG_SIZE)\n",
        "  axes[0].imshow(cropped)\n",
        "  axes[0].axis(\"off\")\n",
        "  axes[0].set_title(\"OCT\")\n",
        "  axes[1].imshow(histology_image_resized)\n",
        "  axes[1].axis(\"off\")\n",
        "  axes[1].set_title(\"Virtual Histology\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "8cf2f634b1edf4fc"
      },
      "id": "8cf2f634b1edf4fc"
    },
    {
      "cell_type": "code",
      "source": [
        "#inject ground truth histology\n",
        "histology_input_image_path = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Raw Data Used In Paper (Paper V2)/LG-19 - Slide04_Section02 (Fig 3.c)/HistologyAligned.tiff\"\n",
        "\n",
        "\n",
        "histology_image = cv2.imread(histology_input_image_path)\n",
        "histology_image = cv2.cvtColor(histology_image, cv2.COLOR_BGR2RGB)\n",
        "cropped_histology = histology_image[z0:z0+height, x0:x0+width]\n",
        "\n",
        "height,width = cropped.shape[:2]\n",
        "histology_image_resized = cv2.resize(cropped_histology, [width,height] , interpolation=cv2.INTER_AREA)\n",
        "visualize_oct2hist_outputs = True\n",
        "if visualize_oct2hist_outputs:\n",
        "  # present side by side\n",
        "  fig, axes = plt.subplots(1, 2, figsize=FIG_SIZE)\n",
        "  axes[0].imshow(cropped)\n",
        "  axes[0].axis(\"off\")\n",
        "  axes[0].set_title(\"OCT\")\n",
        "  axes[1].imshow(histology_image_resized)\n",
        "  axes[1].axis(\"off\")\n",
        "  axes[1].set_title(\"Virtual Histology\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "ByJ8nrZj-ilo"
      },
      "id": "ByJ8nrZj-ilo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#run sam on virtual histology"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d301ca9179966ba1"
      },
      "id": "d301ca9179966ba1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#util functions were taken from https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb#scrollTo=dZSU9BpHr2gc\n",
        "!pip install dataclasses-json\n",
        "!pip install supervision\n",
        "\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Union, Optional\n",
        "from dataclasses_json import dataclass_json\n",
        "from supervision import Detections\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOCategory:\n",
        "    id: int\n",
        "    name: str\n",
        "    supercategory: str\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOImage:\n",
        "    id: int\n",
        "    width: int\n",
        "    height: int\n",
        "    file_name: str\n",
        "    license: int\n",
        "    date_captured: str\n",
        "    coco_url: Optional[str] = None\n",
        "    flickr_url: Optional[str] = None\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOAnnotation:\n",
        "    id: int\n",
        "    image_id: int\n",
        "    category_id: int\n",
        "    segmentation: List[List[float]]\n",
        "    area: float\n",
        "    bbox: Tuple[float, float, float, float]\n",
        "    iscrowd: int\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOLicense:\n",
        "    id: int\n",
        "    name: str\n",
        "    url: str\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOJson:\n",
        "    images: List[COCOImage]\n",
        "    annotations: List[COCOAnnotation]\n",
        "    categories: List[COCOCategory]\n",
        "    licenses: List[COCOLicense]\n",
        "\n",
        "\n",
        "def load_coco_json(json_file: str) -> COCOJson:\n",
        "    import json\n",
        "\n",
        "    with open(json_file, \"r\") as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    return COCOJson.from_dict(json_data)\n",
        "\n",
        "\n",
        "class COCOJsonUtility:\n",
        "    @staticmethod\n",
        "    def get_annotations_by_image_id(coco_data: COCOJson, image_id: int) -> List[COCOAnnotation]:\n",
        "        return [annotation for annotation in coco_data.annotations if annotation.image_id == image_id]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_annotations_by_image_path(coco_data: COCOJson, image_path: str) -> Optional[List[COCOAnnotation]]:\n",
        "        image = COCOJsonUtility.get_image_by_path(coco_data, image_path)\n",
        "        if image:\n",
        "            return COCOJsonUtility.get_annotations_by_image_id(coco_data, image.id)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_image_by_path(coco_data: COCOJson, image_path: str) -> Optional[COCOImage]:\n",
        "        for image in coco_data.images:\n",
        "            if image.file_name == image_path:\n",
        "                return image\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def annotations2detections(annotations: List[COCOAnnotation]) -> Detections:\n",
        "        class_id, xyxy = [], []\n",
        "\n",
        "        for annotation in annotations:\n",
        "            x_min, y_min, width, height = annotation.bbox\n",
        "            class_id.append(annotation.category_id)\n",
        "            xyxy.append([\n",
        "                x_min,\n",
        "                y_min,\n",
        "                x_min + width,\n",
        "                y_min + height\n",
        "            ])\n",
        "\n",
        "        return Detections(\n",
        "            xyxy=np.array(xyxy, dtype=int),\n",
        "            class_id=np.array(class_id, dtype=int)\n",
        "        )"
      ],
      "metadata": {
        "id": "9ce4e4671ec971c9"
      },
      "id": "9ce4e4671ec971c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0b71431"
      },
      "source": [
        "## Environment Set-up"
      ],
      "id": "c0b71431"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47e5a78f"
      },
      "source": [
        "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
      ],
      "id": "47e5a78f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oIegvV8fA1u"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=rf_api_key)\n",
        "project = rf.workspace(rf_workspace).project(rf_project)\n",
        "dataset = project.version(1).download(rf_dataset)"
      ],
      "id": "0oIegvV8fA1u"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "DATA_SET_SUBDIRECTORY = \"test\"\n",
        "ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n",
        "IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n",
        "ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n",
        "print(IMAGES_DIRECTORY_PATH)\n",
        "print(ANNOTATIONS_FILE_PATH)"
      ],
      "metadata": {
        "id": "qCLPtMA-hTR9"
      },
      "id": "qCLPtMA-hTR9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0685a2f5"
      },
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "  print(\"PyTorch version:\", torch.__version__)\n",
        "  print(\"Torchvision version:\", torchvision.__version__)\n",
        "  print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "  import sys\n",
        "  !{sys.executable} -m pip install opencv-python matplotlib\n",
        "  !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "  !mkdir images\n",
        "  !wget -P images https://pbs.twimg.com/media/FvpQj7UWYAAgxfo?format=jpg&name=large\n",
        "#https://twitter.com/JMGardnerMD/status/1655724394805706752/photo/1\n",
        "  !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "id": "0685a2f5"
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv"
      ],
      "metadata": {
        "id": "yty-ZZqTJkG7"
      },
      "id": "yty-ZZqTJkG7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd2bc687"
      },
      "source": [
        "## Set-up"
      ],
      "id": "fd2bc687"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "560725a2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "id": "560725a2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74b6e5f0"
      },
      "outputs": [],
      "source": [
        "def show_anns(anns):\n",
        "\n",
        "  if len(anns) == 0:\n",
        "    return\n",
        "  sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_autoscale_on(False)\n",
        "\n",
        "  img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "  img[:,:,3] = 0\n",
        "  for ann in sorted_anns:\n",
        "      m = ann['segmentation']\n",
        "      color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "      img[m] = color_mask\n",
        "  ax.imshow(img)"
      ],
      "id": "74b6e5f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27c41445"
      },
      "source": [
        "## Example image"
      ],
      "id": "27c41445"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ndp026QrNVHB"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "%matplotlib inline"
      ],
      "id": "Ndp026QrNVHB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8c2824a"
      },
      "source": [
        "## Automatic mask generation"
      ],
      "id": "b8c2824a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ef74c5"
      },
      "source": [
        "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."
      ],
      "id": "d9ef74c5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1848a108"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ],
      "id": "1848a108"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6b1ea21"
      },
      "source": [
        "To generate masks, just run `generate` on an image."
      ],
      "id": "d6b1ea21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "391771c1"
      },
      "outputs": [],
      "source": [
        "masks = mask_generator.generate(histology_image_resized)"
      ],
      "id": "391771c1"
    },
    {
      "cell_type": "code",
      "source": [
        "if visualize_sam_outputs:\n",
        "  plt.figure(figsize=FIG_SIZE)\n",
        "  plt.imshow(histology_image_resized)\n",
        "  show_anns(masks)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "eZbgErUlAqDz"
      },
      "id": "eZbgErUlAqDz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e36a1a39"
      },
      "source": [
        "Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:\n",
        "* `segmentation` : the mask\n",
        "* `area` : the area of the mask in pixels\n",
        "* `bbox` : the boundary box of the mask in XYWH format\n",
        "* `predicted_iou` : the model's own prediction for the quality of the mask\n",
        "* `point_coords` : the sampled input point that generated this mask\n",
        "* `stability_score` : an additional measure of mask quality\n",
        "* `crop_box` : the crop of the image used to generate this mask in XYWH format"
      ],
      "id": "e36a1a39"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53009a1f"
      },
      "source": [
        "Show all the masks overlayed on the image."
      ],
      "id": "53009a1f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00b3d6b2"
      },
      "source": [
        "## Automatic mask generation options"
      ],
      "id": "00b3d6b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "183de84e"
      },
      "source": [
        "There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:[link text](https://)\n"
      ],
      "id": "183de84e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WMuUj8CUJSa"
      },
      "outputs": [],
      "source": [
        "mask_generator_2 = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=points_per_side,\n",
        "    pred_iou_thresh=pred_iou_thresh,\n",
        "    stability_score_thresh=stability_score_thresh,\n",
        "    crop_n_layers=crop_n_layers,\n",
        "    crop_n_points_downscale_factor=crop_n_points_downscale_factor,\n",
        "    min_mask_region_area=min_mask_region_area,\n",
        ")"
      ],
      "id": "4WMuUj8CUJSa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQx93pyFeurs"
      },
      "source": [],
      "id": "PQx93pyFeurs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bebcdaf1"
      },
      "outputs": [],
      "source": [
        "masks2 = mask_generator_2.generate(histology_image_resized)"
      ],
      "id": "bebcdaf1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb702ae3"
      },
      "outputs": [],
      "source": [
        "if visualize_sam_outputs:\n",
        "  plt.figure(figsize=FIG_SIZE)\n",
        "  plt.imshow(image)\n",
        "  show_anns(masks2)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "id": "fb702ae3"
    },
    {
      "cell_type": "code",
      "source": [
        "if visualize_sam_outputs:\n",
        "  boolean_masks = [\n",
        "      masks2['segmentation']\n",
        "      for masks2\n",
        "      in sorted(masks2, key=lambda x: x['area'], reverse=True)\n",
        "  ]\n",
        "\n",
        "  sv.plot_images_grid(\n",
        "      images=boolean_masks,\n",
        "      grid_size=(len(masks), int(len(boolean_masks) / 4)),\n",
        "      size=(16, 16)\n",
        "  )"
      ],
      "metadata": {
        "id": "3VJQrMwsBJQk"
      },
      "id": "3VJQrMwsBJQk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#project on oct"
      ],
      "metadata": {
        "id": "21jMiU5lpQdW"
      },
      "id": "21jMiU5lpQdW"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=FIG_SIZE)\n",
        "plt.imshow(cropped)\n",
        "show_anns(masks2)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m25KFnmrpVeh"
      },
      "id": "m25KFnmrpVeh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}