{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "Use this notebook to convert an OCT image you have to an H&E image in order to evaluate how the code works.\n",
        "\n",
        "To get started,\n",
        "[open this notebook in colab](https://colab.research.google.com/github/WinetraubLab/zero_shot_segmentation/blob/main/zero_shot_segmentation_oct.ipynb)\n",
        " and run.\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "239750a6e2ca3ff1"
      },
      "id": "239750a6e2ca3ff1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# inputs"
      ],
      "metadata": {
        "collapsed": false,
        "id": "cc7aeee9ca83ac5b"
      },
      "id": "cc7aeee9ca83ac5b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Path to an OCT image to convert\n",
        "oct_input_image_path = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Raw Data Used In Paper (Paper V2)/LG-19 - Slide04_Section02 (Fig 3.c)/OCTAligned.tiff\"\n",
        "\n",
        "#how many microns per pixel for each axis\n",
        "microns_per_pixel_z = 1\n",
        "microns_per_pixel_x = 1"
      ],
      "metadata": {
        "id": "effeb7673d4575c4"
      },
      "id": "effeb7673d4575c4"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "XUNJ2Im6sGSJ"
      },
      "id": "XUNJ2Im6sGSJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "visualize_oct2hist_outputs = False"
      ],
      "metadata": {
        "id": "JZj8621TpY5g"
      },
      "id": "JZj8621TpY5g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#rf\n",
        "rf_api_key=\"R04BinsZcBZ6PsfKR2fP\"\n",
        "rf_workspace=\"yolab-kmmfx\"\n",
        "rf_project = \"connect_from_colab\"\n",
        "rf_dataset = \"png-mask-semantic\"\n",
        "#sam\n",
        "using_colab = True\n",
        "visualize_sam_outputs = False"
      ],
      "metadata": {
        "id": "4fe300fb"
      },
      "id": "4fe300fb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#sam\n",
        "points_per_side=32\n",
        "pred_iou_thresh=0.90\n",
        "stability_score_thresh=0.95\n",
        "crop_n_layers=1\n",
        "crop_n_points_downscale_factor=2\n",
        "min_mask_region_area=3000\n",
        "\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "model_type = \"vit_h\""
      ],
      "metadata": {
        "id": "F9UF0zHVUZWA"
      },
      "id": "F9UF0zHVUZWA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assumptions:\n",
        "\n",
        "oct scan x/z rates:\n",
        "*   microns per pixel z = 1\n",
        "*   microns per pixel x = 1\n",
        "\n",
        "pix2pix input sizes:\n",
        "*   virtual histology input width = 256\n",
        "*   virtual histology input height = 256\n",
        "\n",
        "pix2pix input x/z rates:\n",
        "*   microns per pixel z = 1\n",
        "*   microns per pixel x = 2"
      ],
      "metadata": {
        "collapsed": false,
        "id": "uOKT0inNrHLJ"
      },
      "id": "uOKT0inNrHLJ"
    },
    {
      "cell_type": "code",
      "source": [
        "#pix2pix input sizes\n",
        "VIRTUAL_HIST_WIDTH = 256\n",
        "VIRTUAL_HIST_HEIGHT = 256\n",
        "#verify input sizes\n",
        "MICRONS_PER_PIXEL_Z_TARGET = 2\n",
        "MICRONS_PER_PIXEL_X_TARGET = 4"
      ],
      "metadata": {
        "id": "031q6iphqkCP"
      },
      "id": "031q6iphqkCP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#non python, colab setup code"
      ],
      "metadata": {
        "collapsed": false,
        "id": "71a9a28163d8c4c9"
      },
      "id": "71a9a28163d8c4c9"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Clone repository\n",
        "!git clone --recurse-submodules https://github.com/WinetraubLab/OCT2Hist-UseModel\n",
        "\n",
        "base_folder = \"/content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix\"\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -r {base_folder}/requirements.txt\n",
        "\n",
        "# Clean up this window once install is complete\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "niF5tEhPo8lX"
      },
      "id": "niF5tEhPo8lX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# This is the folder that the pre-trained model is in\n",
        "model_folder = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Model (Paper V2)\"\n",
        "\n",
        "# Copy model to this folder over\n",
        "!mkdir {base_folder}/checkpoints\n",
        "!mkdir {base_folder}/checkpoints/pix2pix/\n",
        "!cp \"{model_folder}/latest_net_G.pth\" {base_folder}/checkpoints/pix2pix/\n",
        "!cp \"{model_folder}/latest_net_D.pth\" {base_folder}/checkpoints/pix2pix/"
      ],
      "metadata": {
        "id": "cWY3m7XcpDuz"
      },
      "id": "cWY3m7XcpDuz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess"
      ],
      "metadata": {
        "id": "8UTSh-cIpaMi"
      },
      "id": "8UTSh-cIpaMi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load OCT image\n",
        "oct_image_orig = cv2.imread(oct_input_image_path)\n",
        "oct_image_orig = cv2.cvtColor(oct_image_orig, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "oct_image = oct_image_orig.copy()\n",
        "# Show Images to user\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "oct_image_orig_shape = oct_image.shape\n",
        "axes[0].imshow(oct_image)\n",
        "axes[0].axis(\"off\")\n",
        "axes[0].set_title(f\"Original OCT image ({oct_image_orig_shape})\")\n",
        "\n"
      ],
      "metadata": {
        "id": "990f1eb977d4f9c8"
      },
      "id": "990f1eb977d4f9c8"
    },
    {
      "cell_type": "code",
      "source": [
        "#update the image to be 2mic/pix on the x axis, and 1mic/pix on the y axis.\n",
        "new_image_width = int(oct_image.shape[1] * microns_per_pixel_x / MICRONS_PER_PIXEL_X_TARGET)\n",
        "new_image_height = int(oct_image.shape[0] * microns_per_pixel_z / MICRONS_PER_PIXEL_Z_TARGET)\n",
        "\n",
        "# if new_image_width<VIRTUAL_HIST_WIDTH or VIRTUAL_HIST_HEIGHT<256:\n",
        "#   print(f\"Image at target x/z rate is smaller than ({VIRTUAL_HIST_HEIGHT},{VIRTUAL_HIST_WIDTH}), and will be placed at top left corner.\")\n",
        "# #warn about cropping\n",
        "# if new_image_width>VIRTUAL_HIST_WIDTH or new_image_height>VIRTUAL_HIST_HEIGHT:\n",
        "#   raise Exception(f\"Image at target x/z rate is too large: ({new_image_height},{new_image_width}), please adjust so that it won't be larger than ({VIRTUAL_HIST_HEIGHT},{VIRTUAL_HIST_WIDTH}).\")"
      ],
      "metadata": {
        "id": "P_TXQb-bpMCx"
      },
      "id": "P_TXQb-bpMCx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/OCT2Hist-UseModel\n",
        "from utils.masking_utils import mask_image\n",
        "preprocessed_img, filt_img = mask_image(oct_image)"
      ],
      "metadata": {
        "id": "wWArPI6DplqL"
      },
      "id": "wWArPI6DplqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.img_utils import showImg\n",
        "showImg(preprocessed_img)"
      ],
      "metadata": {
        "id": "TK3SxeM-zgTt"
      },
      "id": "TK3SxeM-zgTt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "crrop"
      ],
      "metadata": {
        "id": "BLCaB29bps8r"
      },
      "id": "BLCaB29bps8r"
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.img_utils import showImg\n",
        "#slice from image\n",
        "width = 256 * 4\n",
        "height = 256 * 2\n",
        "x0 = 135\n",
        "z0= 350\n",
        "cropped = preprocessed_img[z0:z0+height, x0:x0+width]\n",
        "showImg(cropped)\n",
        "resized = cv2.resize(cropped, [VIRTUAL_HIST_WIDTH,VIRTUAL_HIST_HEIGHT] , interpolation=cv2.INTER_AREA)\n",
        "o2h_input = resized"
      ],
      "metadata": {
        "id": "WezE4f4-puKZ"
      },
      "id": "WezE4f4-puKZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "install dependencies required to read the image"
      ],
      "metadata": {
        "id": "Nz8QwLGlodOr"
      },
      "id": "Nz8QwLGlodOr"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a1sJLt-Druw_"
      },
      "id": "a1sJLt-Druw_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "read it and verify it fits the input requirements."
      ],
      "metadata": {
        "id": "w71KS6WUookp"
      },
      "id": "w71KS6WUookp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#run oct2hist"
      ],
      "metadata": {
        "collapsed": false,
        "id": "cf693b48a7221c75"
      },
      "id": "cf693b48a7221c75"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Create a folder and place OCT image\n",
        "!mkdir {base_folder}/dataset\n",
        "!mkdir {base_folder}/dataset/test/\n",
        "\n",
        "# Before writting image to file, check size\n",
        "if o2h_input.shape[:2] != (256, 256):\n",
        "        raise ValueError(\"Image size must be 256x256 pixels to run model on.\")\n",
        "\n",
        "# Padd image and write it to the correct place\n",
        "padded = np.zeros([256,512,3], np.uint8)\n",
        "padded[:,:256,:] = o2h_input[:,:,:]\n",
        "cv2.imwrite(f\"{base_folder}/dataset/test/im1.jpg\", padded)"
      ],
      "metadata": {
        "id": "8d0d1401ba4dc8c2"
      },
      "id": "8d0d1401ba4dc8c2"
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the folder that the pre-trained model is in\n",
        "model_folder = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Model (Paper V2)\"\n",
        "\n",
        "# Copy model to this folder over\n",
        "!mkdir {base_folder}/checkpoints\n",
        "!mkdir {base_folder}/checkpoints/pix2pix/\n",
        "!cp \"{model_folder}/latest_net_G.pth\" {base_folder}/checkpoints/pix2pix/\n",
        "!cp \"{model_folder}/latest_net_D.pth\" {base_folder}/checkpoints/pix2pix/"
      ],
      "metadata": {
        "id": "DUUsOj8Kn4cs"
      },
      "id": "DUUsOj8Kn4cs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "!python {base_folder}/test.py --netG resnet_9blocks --dataroot \"{base_folder}/dataset/\"  --model pix2pix --name pix2pix --checkpoints_dir \"{base_folder}/checkpoints\" --results_dir \"{base_folder}/results\""
      ],
      "metadata": {
        "id": "79dc2be8da20b8f6"
      },
      "id": "79dc2be8da20b8f6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optional: visualize output"
      ],
      "metadata": {
        "id": "eQsbAkR6o4HO"
      },
      "id": "eQsbAkR6o4HO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load the virtual histology image\n",
        "histology_image = cv2.imread(f\"{base_folder}/results/pix2pix/test_latest/images/im1_fake_B.png\")\n",
        "histology_image = cv2.cvtColor(histology_image, cv2.COLOR_BGR2RGB)\n",
        "visualize_oct2hist_outputs = True\n",
        "if visualize_oct2hist_outputs:\n",
        "  # present side by side\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "  axes[0].imshow(oct_image)\n",
        "  axes[0].axis(\"off\")\n",
        "  axes[0].set_title(\"OCT\")\n",
        "  axes[1].imshow(histology_image)\n",
        "  axes[1].axis(\"off\")\n",
        "  axes[1].set_title(\"Virtual Histology\")\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "b20a959b283f04ad"
      },
      "id": "b20a959b283f04ad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "height,width = cropped.shape[:2]\n",
        "histology_image_resized = cv2.resize(histology_image, [width,height] , interpolation=cv2.INTER_AREA)\n",
        "visualize_oct2hist_outputs = True\n",
        "if visualize_oct2hist_outputs:\n",
        "  # present side by side\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
        "  axes[0].imshow(cropped)\n",
        "  axes[0].axis(\"off\")\n",
        "  axes[0].set_title(\"OCT\")\n",
        "  axes[1].imshow(histology_image_resized)\n",
        "  axes[1].axis(\"off\")\n",
        "  axes[1].set_title(\"Virtual Histology\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "8cf2f634b1edf4fc"
      },
      "id": "8cf2f634b1edf4fc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#run sam on virtual histology"
      ],
      "metadata": {
        "collapsed": false,
        "id": "d301ca9179966ba1"
      },
      "id": "d301ca9179966ba1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#util functions were taken from https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb#scrollTo=dZSU9BpHr2gc\n",
        "!pip install dataclasses-json\n",
        "!pip install supervision\n",
        "\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Union, Optional\n",
        "from dataclasses_json import dataclass_json\n",
        "from supervision import Detections\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOCategory:\n",
        "    id: int\n",
        "    name: str\n",
        "    supercategory: str\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOImage:\n",
        "    id: int\n",
        "    width: int\n",
        "    height: int\n",
        "    file_name: str\n",
        "    license: int\n",
        "    date_captured: str\n",
        "    coco_url: Optional[str] = None\n",
        "    flickr_url: Optional[str] = None\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOAnnotation:\n",
        "    id: int\n",
        "    image_id: int\n",
        "    category_id: int\n",
        "    segmentation: List[List[float]]\n",
        "    area: float\n",
        "    bbox: Tuple[float, float, float, float]\n",
        "    iscrowd: int\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOLicense:\n",
        "    id: int\n",
        "    name: str\n",
        "    url: str\n",
        "\n",
        "\n",
        "@dataclass_json\n",
        "@dataclass\n",
        "class COCOJson:\n",
        "    images: List[COCOImage]\n",
        "    annotations: List[COCOAnnotation]\n",
        "    categories: List[COCOCategory]\n",
        "    licenses: List[COCOLicense]\n",
        "\n",
        "\n",
        "def load_coco_json(json_file: str) -> COCOJson:\n",
        "    import json\n",
        "\n",
        "    with open(json_file, \"r\") as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    return COCOJson.from_dict(json_data)\n",
        "\n",
        "\n",
        "class COCOJsonUtility:\n",
        "    @staticmethod\n",
        "    def get_annotations_by_image_id(coco_data: COCOJson, image_id: int) -> List[COCOAnnotation]:\n",
        "        return [annotation for annotation in coco_data.annotations if annotation.image_id == image_id]\n",
        "\n",
        "    @staticmethod\n",
        "    def get_annotations_by_image_path(coco_data: COCOJson, image_path: str) -> Optional[List[COCOAnnotation]]:\n",
        "        image = COCOJsonUtility.get_image_by_path(coco_data, image_path)\n",
        "        if image:\n",
        "            return COCOJsonUtility.get_annotations_by_image_id(coco_data, image.id)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    @staticmethod\n",
        "    def get_image_by_path(coco_data: COCOJson, image_path: str) -> Optional[COCOImage]:\n",
        "        for image in coco_data.images:\n",
        "            if image.file_name == image_path:\n",
        "                return image\n",
        "        return None\n",
        "\n",
        "    @staticmethod\n",
        "    def annotations2detections(annotations: List[COCOAnnotation]) -> Detections:\n",
        "        class_id, xyxy = [], []\n",
        "\n",
        "        for annotation in annotations:\n",
        "            x_min, y_min, width, height = annotation.bbox\n",
        "            class_id.append(annotation.category_id)\n",
        "            xyxy.append([\n",
        "                x_min,\n",
        "                y_min,\n",
        "                x_min + width,\n",
        "                y_min + height\n",
        "            ])\n",
        "\n",
        "        return Detections(\n",
        "            xyxy=np.array(xyxy, dtype=int),\n",
        "            class_id=np.array(class_id, dtype=int)\n",
        "        )"
      ],
      "metadata": {
        "id": "9ce4e4671ec971c9"
      },
      "id": "9ce4e4671ec971c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0b71431"
      },
      "source": [
        "## Environment Set-up"
      ],
      "id": "c0b71431"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47e5a78f"
      },
      "source": [
        "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
      ],
      "id": "47e5a78f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oIegvV8fA1u"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=rf_api_key)\n",
        "project = rf.workspace(rf_workspace).project(rf_project)\n",
        "dataset = project.version(1).download(rf_dataset)"
      ],
      "id": "0oIegvV8fA1u"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "DATA_SET_SUBDIRECTORY = \"test\"\n",
        "ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n",
        "IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n",
        "ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n",
        "print(IMAGES_DIRECTORY_PATH)\n",
        "print(ANNOTATIONS_FILE_PATH)"
      ],
      "metadata": {
        "id": "qCLPtMA-hTR9"
      },
      "id": "qCLPtMA-hTR9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0685a2f5"
      },
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "  print(\"PyTorch version:\", torch.__version__)\n",
        "  print(\"Torchvision version:\", torchvision.__version__)\n",
        "  print(\"CUDA is available:\", torch.cuda.is_available())\n",
        "  import sys\n",
        "  !{sys.executable} -m pip install opencv-python matplotlib\n",
        "  !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "\n",
        "  !mkdir images\n",
        "  !wget -P images https://pbs.twimg.com/media/FvpQj7UWYAAgxfo?format=jpg&name=large\n",
        "#https://twitter.com/JMGardnerMD/status/1655724394805706752/photo/1\n",
        "  !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
      ],
      "id": "0685a2f5"
    },
    {
      "cell_type": "code",
      "source": [
        "import supervision as sv"
      ],
      "metadata": {
        "id": "yty-ZZqTJkG7"
      },
      "id": "yty-ZZqTJkG7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd2bc687"
      },
      "source": [
        "## Set-up"
      ],
      "id": "fd2bc687"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "560725a2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "id": "560725a2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74b6e5f0"
      },
      "outputs": [],
      "source": [
        "def show_anns(anns):\n",
        "\n",
        "  if len(anns) == 0:\n",
        "    return\n",
        "  sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_autoscale_on(False)\n",
        "\n",
        "  img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
        "  img[:,:,3] = 0\n",
        "  for ann in sorted_anns:\n",
        "      m = ann['segmentation']\n",
        "      color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
        "      img[m] = color_mask\n",
        "  ax.imshow(img)"
      ],
      "id": "74b6e5f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27c41445"
      },
      "source": [
        "## Example image"
      ],
      "id": "27c41445"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ndp026QrNVHB"
      },
      "outputs": [],
      "source": [
        "%matplotlib notebook\n",
        "%matplotlib inline"
      ],
      "id": "Ndp026QrNVHB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ad354922"
      },
      "outputs": [],
      "source": [
        "image = histology_image_resized"
      ],
      "id": "ad354922"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8c2824a"
      },
      "source": [
        "## Automatic mask generation"
      ],
      "id": "b8c2824a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ef74c5"
      },
      "source": [
        "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."
      ],
      "id": "d9ef74c5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1848a108"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"..\")\n",
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "device = \"cuda\"\n",
        "\n",
        "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "sam.to(device=device)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ],
      "id": "1848a108"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6b1ea21"
      },
      "source": [
        "To generate masks, just run `generate` on an image."
      ],
      "id": "d6b1ea21"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "391771c1"
      },
      "outputs": [],
      "source": [
        "masks = mask_generator.generate(image)"
      ],
      "id": "391771c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e36a1a39"
      },
      "source": [
        "Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:\n",
        "* `segmentation` : the mask\n",
        "* `area` : the area of the mask in pixels\n",
        "* `bbox` : the boundary box of the mask in XYWH format\n",
        "* `predicted_iou` : the model's own prediction for the quality of the mask\n",
        "* `point_coords` : the sampled input point that generated this mask\n",
        "* `stability_score` : an additional measure of mask quality\n",
        "* `crop_box` : the crop of the image used to generate this mask in XYWH format"
      ],
      "id": "e36a1a39"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53009a1f"
      },
      "source": [
        "Show all the masks overlayed on the image."
      ],
      "id": "53009a1f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00b3d6b2"
      },
      "source": [
        "## Automatic mask generation options"
      ],
      "id": "00b3d6b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "183de84e"
      },
      "source": [
        "There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:[link text](https://)\n"
      ],
      "id": "183de84e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WMuUj8CUJSa"
      },
      "outputs": [],
      "source": [
        "mask_generator_2 = SamAutomaticMaskGenerator(\n",
        "    model=sam,\n",
        "    points_per_side=points_per_side,\n",
        "    pred_iou_thresh=pred_iou_thresh,\n",
        "    stability_score_thresh=stability_score_thresh,\n",
        "    crop_n_layers=crop_n_layers,\n",
        "    crop_n_points_downscale_factor=crop_n_points_downscale_factor,\n",
        "    min_mask_region_area=min_mask_region_area,\n",
        ")"
      ],
      "id": "4WMuUj8CUJSa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQx93pyFeurs"
      },
      "source": [],
      "id": "PQx93pyFeurs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bebcdaf1"
      },
      "outputs": [],
      "source": [
        "masks2 = mask_generator_2.generate(image)"
      ],
      "id": "bebcdaf1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb702ae3"
      },
      "outputs": [],
      "source": [
        "if visualize_sam_outputs:\n",
        "  plt.figure(figsize=(15,15))\n",
        "  plt.imshow(image)\n",
        "  show_anns(masks2)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "id": "fb702ae3"
    },
    {
      "cell_type": "code",
      "source": [
        "if visualize_sam_outputs:\n",
        "  boolean_masks = [\n",
        "      masks2['segmentation']\n",
        "      for masks2\n",
        "      in sorted(masks2, key=lambda x: x['area'], reverse=True)\n",
        "  ]\n",
        "\n",
        "  sv.plot_images_grid(\n",
        "      images=boolean_masks,\n",
        "      grid_size=(len(masks), int(len(boolean_masks) / 4)),\n",
        "      size=(16, 16)\n",
        "  )"
      ],
      "metadata": {
        "id": "3VJQrMwsBJQk"
      },
      "id": "3VJQrMwsBJQk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#project on oct"
      ],
      "metadata": {
        "id": "21jMiU5lpQdW"
      },
      "id": "21jMiU5lpQdW"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(cropped)\n",
        "show_anns(masks2)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m25KFnmrpVeh"
      },
      "id": "m25KFnmrpVeh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}