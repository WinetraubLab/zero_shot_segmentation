{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "5fa21d44"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ],
   "id": "5fa21d44"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "Use this notebook to convert an OCT image you have to an H&E image in order to evaluate how the code works.\n",
    "\n",
    "To get started,\n",
    "[open this notebook in colab](https://colab.research.google.com/github/WinetraubLab/zero_shot_segmentation/blob/main/zero_shot_segmentation_oct.ipynb)\n",
    " and run.\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "239750a6e2ca3ff1"
   },
   "id": "239750a6e2ca3ff1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# inputs"
   ],
   "metadata": {
    "collapsed": false,
    "id": "cc7aeee9ca83ac5b"
   },
   "id": "cc7aeee9ca83ac5b"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# Path to an OCT image to convert\n",
    "oct_input_image_path = \"/content/drive/Shareddrives/Yolab - Current Projects/Emilie/OCT2Hist test Library/cropped image.png\"\n",
    "base_folder = \"/content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "effeb7673d4575c4"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "visualize_oct2hist_outputs = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "JZj8621TpY5g"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "#rf\n",
    "rf_api_key=\"R04BinsZcBZ6PsfKR2fP\"\n",
    "rf_workspace=\"yolab-kmmfx\"\n",
    "rf_project = \"connect_from_colab\"\n",
    "rf_dataset = \"png-mask-semantic\"\n",
    "#sam\n",
    "using_colab = True\n",
    "visualize_sam_outputs = False"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fe300fb"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "#sam\n",
    "points_per_side=32\n",
    "pred_iou_thresh=0.90\n",
    "stability_score_thresh=0.95\n",
    "crop_n_layers=1\n",
    "crop_n_points_downscale_factor=2\n",
    "min_mask_region_area=3000\n",
    "\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "F9UF0zHVUZWA"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assumptions:\n",
    "\n",
    "oct scan x/z rates:\n",
    "*   microns per pixel z = 1\n",
    "*   microns per pixel x = 1\n",
    "\n",
    "pix2pix input sizes:\n",
    "*   virtual histology input width = 256\n",
    "*   virtual histology input height = 256\n",
    "\n",
    "pix2pix input x/z rates:\n",
    "*   microns per pixel z = 1\n",
    "*   microns per pixel x = 2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "uOKT0inNrHLJ"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#non python, colab setup code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71a9a28163d8c4c9"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "!git clone --recurse-submodules https://github.com/WinetraubLab/OCT2Hist-UseModel\n",
    "\n",
    "!pip install -r {base_folder}/requirements.txt\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "%cd /content/OCT2Hist-UseModel\n"
   ],
   "metadata": {
    "id": "aa49b9b366740041"
   },
   "id": "aa49b9b366740041"
  },
  {
   "cell_type": "markdown",
   "source": [
    "install dependencies required to read the image"
   ],
   "metadata": {
    "id": "Nz8QwLGlodOr"
   },
   "id": "Nz8QwLGlodOr"
  },
  {
   "cell_type": "markdown",
   "source": [
    "read it and verify it fits the input requirements."
   ],
   "metadata": {
    "id": "w71KS6WUookp"
   },
   "id": "w71KS6WUookp"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/OCT2Hist-UseModel\n"
     ]
    }
   ],
   "source": [
    "from utils.use_oct_utils import oct_get_image_and_preprocess\n",
    "oct_get_image_and_preprocess(oct_input_image_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "990f1eb977d4f9c8",
    "outputId": "994ffcd9-8cf7-458b-c9d4-1643957c6a69"
   },
   "id": "990f1eb977d4f9c8"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3bfdacffe79f8626"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#run oct2hist"
   ],
   "metadata": {
    "collapsed": false,
    "id": "cf693b48a7221c75"
   },
   "id": "cf693b48a7221c75"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘/content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/dataset’: File exists\n",
      "mkdir: cannot create directory ‘/content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/dataset/test/’: File exists\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "# Create a folder and place OCT image\n",
    "!mkdir {base_folder}/dataset\n",
    "!mkdir {base_folder}/dataset/test/\n",
    "\n",
    "# Before writting image to file, check size\n",
    "if o2h_input.shape[:2] != (256, 256):\n",
    "        raise ValueError(\"Image size must be 256x256 pixels to run model on.\")\n",
    "\n",
    "# Padd image and write it to the correct place\n",
    "padded = np.zeros([256,512,3], np.uint8)\n",
    "padded[:,:256,:] = o2h_input[:,:,:]\n",
    "cv2.imwrite(f\"{base_folder}/dataset/test/im1.jpg\", padded)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d0d1401ba4dc8c2",
    "outputId": "1876703b-ae4d-4901-de3d-16eaf6fb3232"
   },
   "id": "8d0d1401ba4dc8c2"
  },
  {
   "cell_type": "code",
   "source": [
    "# This is the folder that the pre-trained model is in\n",
    "model_folder = \"/content/drive/Shareddrives/Yolab - Current Projects/_Datasets/2020-11-10 10x Model (Paper V2)\"\n",
    "\n",
    "# Copy model to this folder over\n",
    "!mkdir {base_folder}/checkpoints\n",
    "!mkdir {base_folder}/checkpoints/pix2pix/\n",
    "!cp \"{model_folder}/latest_net_G.pth\" {base_folder}/checkpoints/pix2pix/\n",
    "!cp \"{model_folder}/latest_net_D.pth\" {base_folder}/checkpoints/pix2pix/"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUUsOj8Kn4cs",
    "outputId": "2fa2ec3e-f0fb-42e4-9db0-cbbbcd1439c6"
   },
   "id": "DUUsOj8Kn4cs",
   "execution_count": 68,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘/content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/checkpoints’: File exists\n",
      "mkdir: cannot create directory ‘/content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/checkpoints/pix2pix/’: File exists\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------- Options ---------------\n",
      "             aspect_ratio: 1.0                           \n",
      "               batch_size: 1                             \n",
      "          checkpoints_dir: /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/checkpoints\t[default: ./checkpoints]\n",
      "                crop_size: 256                           \n",
      "                 dataroot: /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/dataset/\t[default: None]\n",
      "             dataset_mode: aligned                       \n",
      "                direction: AtoB                          \n",
      "          display_winsize: 256                           \n",
      "                    epoch: latest                        \n",
      "                     eval: False                         \n",
      "                  gpu_ids: 0                             \n",
      "                init_gain: 0.02                          \n",
      "                init_type: normal                        \n",
      "                 input_nc: 3                             \n",
      "                  isTrain: False                         \t[default: None]\n",
      "                load_iter: 0                             \t[default: 0]\n",
      "                load_size: 256                           \n",
      "         max_dataset_size: inf                           \n",
      "                    model: pix2pix                       \t[default: test]\n",
      "               n_layers_D: 3                             \n",
      "                     name: pix2pix                       \t[default: experiment_name]\n",
      "                      ndf: 64                            \n",
      "                     netD: basic                         \n",
      "                     netG: resnet_9blocks                \t[default: unet_256]\n",
      "                      ngf: 64                            \n",
      "               no_dropout: False                         \n",
      "                  no_flip: False                         \n",
      "                     norm: batch                         \n",
      "                 num_test: 50                            \n",
      "              num_threads: 4                             \n",
      "                output_nc: 3                             \n",
      "                    phase: test                          \n",
      "               preprocess: resize_and_crop               \n",
      "              results_dir: /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/results\t[default: ./results/]\n",
      "           serial_batches: False                         \n",
      "                   suffix:                               \n",
      "                use_wandb: False                         \n",
      "                  verbose: False                         \n",
      "       wandb_project_name: CycleGAN-and-pix2pix          \n",
      "----------------- End -------------------\n",
      "dataset [AlignedDataset] was created\n",
      "initialize network with normal\n",
      "model [Pix2PixModel] was created\n",
      "loading the model from /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/checkpoints/pix2pix/latest_net_G.pth\n",
      "---------- Networks initialized -------------\n",
      "[Network G] Total number of parameters : 11.383 M\n",
      "-----------------------------------------------\n",
      "creating web directory /content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/results/pix2pix/test_latest\n",
      "processing (0000)-th image... ['/content/OCT2Hist-UseModel/pytorch-CycleGAN-and-pix2pix/dataset/test/im1.jpg']\n"
     ]
    }
   ],
   "source": [
    "!python {base_folder}/test.py --netG resnet_9blocks --dataroot \"{base_folder}/dataset/\"  --model pix2pix --name pix2pix --checkpoints_dir \"{base_folder}/checkpoints\" --results_dir \"{base_folder}/results\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79dc2be8da20b8f6",
    "outputId": "3bbdfb64-cb15-4118-c021-279ae33d2c65"
   },
   "id": "79dc2be8da20b8f6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Optional: visualize output"
   ],
   "metadata": {
    "id": "eQsbAkR6o4HO"
   },
   "id": "eQsbAkR6o4HO"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# Load the virtual histology image\n",
    "histology_image = cv2.imread(f\"{base_folder}/results/pix2pix/test_latest/images/im1_fake_B.png\")\n",
    "histology_image = cv2.cvtColor(histology_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "if visualize_oct2hist_outputs:\n",
    "  # present side by side\n",
    "  fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "  axes[0].imshow(oct_image)\n",
    "  axes[0].axis(\"off\")\n",
    "  axes[0].set_title(\"OCT\")\n",
    "  axes[1].imshow(histology_image)\n",
    "  axes[1].axis(\"off\")\n",
    "  axes[1].set_title(\"Virtual Histology\")\n",
    "  plt.show()\n"
   ],
   "metadata": {
    "id": "b20a959b283f04ad"
   },
   "id": "b20a959b283f04ad"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "height,width = preprocessed_img.shape[:2]\n",
    "histology_image_resized = cv2.resize(histology_image, [width,height] , interpolation=cv2.INTER_AREA)\n",
    "\n",
    "if visualize_oct2hist_outputs:\n",
    "  # present side by side\n",
    "  fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "  axes[0].imshow(preprocessed_img)\n",
    "  axes[0].axis(\"off\")\n",
    "  axes[0].set_title(\"OCT\")\n",
    "  axes[1].imshow(histology_image_resized)\n",
    "  axes[1].axis(\"off\")\n",
    "  axes[1].set_title(\"Virtual Histology\")\n",
    "  plt.show()"
   ],
   "metadata": {
    "id": "8cf2f634b1edf4fc"
   },
   "id": "8cf2f634b1edf4fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#run sam on virtual histology"
   ],
   "metadata": {
    "collapsed": false,
    "id": "d301ca9179966ba1"
   },
   "id": "d301ca9179966ba1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (0.5.14)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json) (0.9.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json) (4.7.1)\n"
     ]
    }
   ],
   "source": [
    "#util functions were taken from https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/how-to-segment-anything-with-sam.ipynb#scrollTo=dZSU9BpHr2gc\n",
    "!pip install dataclasses-json\n",
    "!pip install supervision\n",
    "\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Union, Optional\n",
    "from dataclasses_json import dataclass_json\n",
    "from supervision import Detections\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOCategory:\n",
    "    id: int\n",
    "    name: str\n",
    "    supercategory: str\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOImage:\n",
    "    id: int\n",
    "    width: int\n",
    "    height: int\n",
    "    file_name: str\n",
    "    license: int\n",
    "    date_captured: str\n",
    "    coco_url: Optional[str] = None\n",
    "    flickr_url: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOAnnotation:\n",
    "    id: int\n",
    "    image_id: int\n",
    "    category_id: int\n",
    "    segmentation: List[List[float]]\n",
    "    area: float\n",
    "    bbox: Tuple[float, float, float, float]\n",
    "    iscrowd: int\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOLicense:\n",
    "    id: int\n",
    "    name: str\n",
    "    url: str\n",
    "\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class COCOJson:\n",
    "    images: List[COCOImage]\n",
    "    annotations: List[COCOAnnotation]\n",
    "    categories: List[COCOCategory]\n",
    "    licenses: List[COCOLicense]\n",
    "\n",
    "\n",
    "def load_coco_json(json_file: str) -> COCOJson:\n",
    "    import json\n",
    "\n",
    "    with open(json_file, \"r\") as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    return COCOJson.from_dict(json_data)\n",
    "\n",
    "\n",
    "class COCOJsonUtility:\n",
    "    @staticmethod\n",
    "    def get_annotations_by_image_id(coco_data: COCOJson, image_id: int) -> List[COCOAnnotation]:\n",
    "        return [annotation for annotation in coco_data.annotations if annotation.image_id == image_id]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_annotations_by_image_path(coco_data: COCOJson, image_path: str) -> Optional[List[COCOAnnotation]]:\n",
    "        image = COCOJsonUtility.get_image_by_path(coco_data, image_path)\n",
    "        if image:\n",
    "            return COCOJsonUtility.get_annotations_by_image_id(coco_data, image.id)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_image_by_path(coco_data: COCOJson, image_path: str) -> Optional[COCOImage]:\n",
    "        for image in coco_data.images:\n",
    "            if image.file_name == image_path:\n",
    "                return image\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def annotations2detections(annotations: List[COCOAnnotation]) -> Detections:\n",
    "        class_id, xyxy = [], []\n",
    "\n",
    "        for annotation in annotations:\n",
    "            x_min, y_min, width, height = annotation.bbox\n",
    "            class_id.append(annotation.category_id)\n",
    "            xyxy.append([\n",
    "                x_min,\n",
    "                y_min,\n",
    "                x_min + width,\n",
    "                y_min + height\n",
    "            ])\n",
    "\n",
    "        return Detections(\n",
    "            xyxy=np.array(xyxy, dtype=int),\n",
    "            class_id=np.array(class_id, dtype=int)\n",
    "        )"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ce4e4671ec971c9",
    "outputId": "61c8cbe2-a857-46b6-c4aa-3fee3a9e99ad"
   },
   "id": "9ce4e4671ec971c9"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0b71431"
   },
   "source": [
    "## Environment Set-up"
   ],
   "id": "c0b71431"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47e5a78f"
   },
   "source": [
    "If running locally using jupyter, first install `segment_anything` in your environment using the [installation instructions](https://github.com/facebookresearch/segment-anything#installation) in the repository. If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'."
   ],
   "id": "47e5a78f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oIegvV8fA1u"
   },
   "outputs": [],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=rf_api_key)\n",
    "project = rf.workspace(rf_workspace).project(rf_project)\n",
    "dataset = project.version(1).download(rf_dataset)"
   ],
   "id": "0oIegvV8fA1u"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "DATA_SET_SUBDIRECTORY = \"test\"\n",
    "ANNOTATIONS_FILE_NAME = \"_annotations.coco.json\"\n",
    "IMAGES_DIRECTORY_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY)\n",
    "ANNOTATIONS_FILE_PATH = os.path.join(dataset.location, DATA_SET_SUBDIRECTORY, ANNOTATIONS_FILE_NAME)\n",
    "print(IMAGES_DIRECTORY_PATH)\n",
    "print(ANNOTATIONS_FILE_PATH)"
   ],
   "metadata": {
    "id": "qCLPtMA-hTR9"
   },
   "id": "qCLPtMA-hTR9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0685a2f5"
   },
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "  print(\"PyTorch version:\", torch.__version__)\n",
    "  print(\"Torchvision version:\", torchvision.__version__)\n",
    "  print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "  import sys\n",
    "  !{sys.executable} -m pip install opencv-python matplotlib\n",
    "  !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "\n",
    "  !mkdir images\n",
    "  !wget -P images https://pbs.twimg.com/media/FvpQj7UWYAAgxfo?format=jpg&name=large\n",
    "#https://twitter.com/JMGardnerMD/status/1655724394805706752/photo/1\n",
    "  !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
   ],
   "id": "0685a2f5"
  },
  {
   "cell_type": "code",
   "source": [
    "import supervision as sv"
   ],
   "metadata": {
    "id": "yty-ZZqTJkG7"
   },
   "id": "yty-ZZqTJkG7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd2bc687"
   },
   "source": [
    "## Set-up"
   ],
   "id": "fd2bc687"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "560725a2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ],
   "id": "560725a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74b6e5f0"
   },
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "\n",
    "  if len(anns) == 0:\n",
    "    return\n",
    "  sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "  ax = plt.gca()\n",
    "  ax.set_autoscale_on(False)\n",
    "\n",
    "  img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "  img[:,:,3] = 0\n",
    "  for ann in sorted_anns:\n",
    "      m = ann['segmentation']\n",
    "      color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "      img[m] = color_mask\n",
    "  ax.imshow(img)"
   ],
   "id": "74b6e5f0"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27c41445"
   },
   "source": [
    "## Example image"
   ],
   "id": "27c41445"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ndp026QrNVHB"
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ],
   "id": "Ndp026QrNVHB"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ad354922"
   },
   "outputs": [],
   "source": [
    "image = histology_image_resized"
   ],
   "id": "ad354922"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0ac8c67"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "e0ac8c67"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8c2824a"
   },
   "source": [
    "## Automatic mask generation"
   ],
   "id": "b8c2824a"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9ef74c5"
   },
   "source": [
    "To run automatic mask generation, provide a SAM model to the `SamAutomaticMaskGenerator` class. Set the path below to the SAM checkpoint. Running on CUDA and with the default model is recommended."
   ],
   "id": "d9ef74c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1848a108"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ],
   "id": "1848a108"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6b1ea21"
   },
   "source": [
    "To generate masks, just run `generate` on an image."
   ],
   "id": "d6b1ea21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "391771c1"
   },
   "outputs": [],
   "source": [
    "masks = mask_generator.generate(image)"
   ],
   "id": "391771c1"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e36a1a39"
   },
   "source": [
    "Mask generation returns a list over masks, where each mask is a dictionary containing various data about the mask. These keys are:\n",
    "* `segmentation` : the mask\n",
    "* `area` : the area of the mask in pixels\n",
    "* `bbox` : the boundary box of the mask in XYWH format\n",
    "* `predicted_iou` : the model's own prediction for the quality of the mask\n",
    "* `point_coords` : the sampled input point that generated this mask\n",
    "* `stability_score` : an additional measure of mask quality\n",
    "* `crop_box` : the crop of the image used to generate this mask in XYWH format"
   ],
   "id": "e36a1a39"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53009a1f"
   },
   "source": [
    "Show all the masks overlayed on the image."
   ],
   "id": "53009a1f"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00b3d6b2"
   },
   "source": [
    "## Automatic mask generation options"
   ],
   "id": "00b3d6b2"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "183de84e"
   },
   "source": [
    "There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:[link text](https://)\n"
   ],
   "id": "183de84e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4WMuUj8CUJSa"
   },
   "outputs": [],
   "source": [
    "mask_generator_2 = SamAutomaticMaskGenerator(\n",
    "    model=sam,\n",
    "    points_per_side=points_per_side,\n",
    "    pred_iou_thresh=pred_iou_thresh,\n",
    "    stability_score_thresh=stability_score_thresh,\n",
    "    crop_n_layers=crop_n_layers,\n",
    "    crop_n_points_downscale_factor=crop_n_points_downscale_factor,\n",
    "    min_mask_region_area=min_mask_region_area,\n",
    ")"
   ],
   "id": "4WMuUj8CUJSa"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQx93pyFeurs"
   },
   "source": [],
   "id": "PQx93pyFeurs"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bebcdaf1"
   },
   "outputs": [],
   "source": [
    "masks2 = mask_generator_2.generate(image)"
   ],
   "id": "bebcdaf1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb702ae3"
   },
   "outputs": [],
   "source": [
    "if visualize_sam_outputs:\n",
    "  plt.figure(figsize=(15,15))\n",
    "  plt.imshow(image)\n",
    "  show_anns(masks2)\n",
    "  plt.axis('off')\n",
    "  plt.show()"
   ],
   "id": "fb702ae3"
  },
  {
   "cell_type": "code",
   "source": [
    "if visualize_sam_outputs:\n",
    "  boolean_masks = [\n",
    "      masks2['segmentation']\n",
    "      for masks2\n",
    "      in sorted(masks2, key=lambda x: x['area'], reverse=True)\n",
    "  ]\n",
    "\n",
    "  sv.plot_images_grid(\n",
    "      images=boolean_masks,\n",
    "      grid_size=(len(masks), int(len(boolean_masks) / 4)),\n",
    "      size=(16, 16)\n",
    "  )"
   ],
   "metadata": {
    "id": "3VJQrMwsBJQk"
   },
   "id": "3VJQrMwsBJQk",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#project on oct"
   ],
   "metadata": {
    "id": "21jMiU5lpQdW"
   },
   "id": "21jMiU5lpQdW"
  },
  {
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(oct_image)\n",
    "show_anns(masks2)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "m25KFnmrpVeh"
   },
   "id": "m25KFnmrpVeh",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
